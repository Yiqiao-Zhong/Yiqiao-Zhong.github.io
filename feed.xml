<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-21T00:53:00-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Advancing Interpretability of Deep Learning</title><subtitle>Can we understand the inner workings of black-box models? The goal of the blogs is to explore structures and analyze empirical phenoemna by scientific experiments on deep learning. </subtitle><entry><title type="html">Can LLMs solve novel tasks? Induction heads, composition, and out-of-distribution generalization</title><link href="http://localhost:4000/jekyll/update/2025/02/18/induction-heads.html" rel="alternate" type="text/html" title="Can LLMs solve novel tasks? Induction heads, composition, and out-of-distribution generalization" /><published>2025-02-18T21:00:00-06:00</published><updated>2025-02-18T21:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2025/02/18/induction-heads</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/18/induction-heads.html"><![CDATA[<p>This post is based on my recent <a href="https://www.pnas.org/doi/10.1073/pnas.2417182122">paper</a> published in PNAS. If you can’t get past the paywall, here is a <a href="https://arxiv.org/abs/2408.09503">arXiv version</a>.
Big thanks to my collaborator <a href="https://github.com/JiajunSong629">Jiajun Song</a> and <a href="https://pages.cs.wisc.edu/~zxu444/home/">Zhuoyan Xu</a>!</p>

<h2 id="can-llms-solve-novelty-tasks-the-agi-debate">Can LLMs solve novelty tasks? The AGI debate</h2>

<p>Since ChatGPT, there have been a lot of polarizing opinons about LLMs. One focal point is the novelty of the generated texts by an LLM.</p>

<p>Proponents point to the examples of creative solutions produced by ChatGPT and Claude, hailing these models as AGI or superhuman intelligence. Others argue that these models are simply parroting internet text and incapable of generating truly original generation.</p>

<p>Well…it all depends on how you define novelty and creativity, in my opinon. In statistics, a standard and well-studied situation is to train a model on data drawn from the training distribution \(P_{\mathrm{train}}\) and evaluate it on the same test distribution \(P_{\mathrm{test}}\). 
This classical notation of generalization requires training a model for each specific task. As many researchers know, GPT-3 and ChatGPT are smarter than that—which is epitomized by the <a href="https://arxiv.org/abs/2005.14165"><strong>in-context learning</strong></a> (ICL).</p>

<p>Here is a simple proposal to study “novelty” of models, which is often called out-of-distribution (OOD) generalization:
\(\begin{aligned}
P_{\mathrm{train}} \neq P_{\mathrm{test}}.
\end{aligned}\)</p>

<h2 id="llms-generalize-on-certain-reasoning-tasks">LLMs generalize on certain reasoning tasks</h2>

<p>Let’s say someone asks you to play a game: given a prompt</p>
<blockquote>
  <p>“Then, <em>Henry</em> and <em>Blake</em> had a long argument. Afterwards <em>Henry</em> said to”</p>
</blockquote>

<p>what is the next probable word? It is easy to guess Blake as the natural continuation. Now what if the prompt is changed a bit?</p>
<blockquote>
  <p>“Then, \&amp;\^ and #$ had a long argument. Afterwards \&amp;\^ said to”</p>
</blockquote>

<p>It may take you some time to realize that I simply replaced the names with special symbols. Once seeing this point, you probably say #$.</p>

<p>Another example of symbolic replacement is a variant of <strong>ICL</strong>:</p>
<blockquote>
  <p>baseball is $#, celery is !\%, sheep is \&amp;*, volleyball is $#, lettuce is</p>
</blockquote>

<p>This prompt asks you to classifying objects into three classes, which are represented by…well…strange symbols. If you follow the replacement logic, then you will probably guess the natural continuation !\%.</p>

<p>While LLMs (especially earlier versions) are unlikely to have seen these strange artificially modified sentences during pretraining, we do see a point in these examples. Training data are mixed with natural languages, math expression, code, and other formatted text such as html. Testing models with perturbed prompts help us understand what models can do and cannot do.</p>

<p>A simple experiment shows how some of the open-source LLMs perform on these two tasks, with and without symbol replacement.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/table1.png" alt="A first look at OOD generalization" /></p>

<p>So, do LLMs solve novel tasks? I’d say yes, because the models seem to have figured out (not perfectly but definitely better than random guess) the “replacement rule” even when they are not explicitly trained on the above specific examples. Instances of the similar nature are observed in the literature, promoting the <a href="https://arxiv.org/abs/2303.12712">“Spark of AGI”</a> argument.</p>

<h2 id="representation-of-concepts-and-linear-representation-hypothesis-lrh">Representation of concepts and linear representation hypothesis (LRH)</h2>

<p>Can we undestand the model’s mechanism a bit better? In the past two years, a lot of interpretability analyses focus on the representation of concepts within a Transformer, most notably the <strong>sparse autoencoder</strong> (SAE) papers, one from <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/">Anthropic</a> and another from <a href="https://cdn.openai.com/papers/sparse-autoencoders.pdf">OpenAI</a> last year. These papers examine the embeddings (or hidden states) of a prompt, which is a simply a vector in the Euclidean space. My <a href="/_site/jekyll/update/2023/10/28/welcome-to-jekyll.html">previous blog</a> also explores the geometry of embeddings across network layers.</p>

<p>A key observation is the following:</p>
<blockquote>
  <p>Concepts are represented as linear subspaces in the embedding space.</p>
</blockquote>

<p>Let me give you an example of this. Suppose that I have a prompt that contains the word “apple” as the last word. Now if I pass the prompt into an LLM, how does the embedding vector looks like?</p>

<p>Representing a word or a prompt as a linear combination of base concepts is an important intution for explaining the inner workings of LLMs. It is often referred to as the <a href="https://arxiv.org/abs/2311.03658"><strong>linear representation hypothesis</strong></a> (LRH).</p>

\[\begin{aligned}
\text{apple} = 0.09 \text{dessert} + 0.11 \text{organism} + 0.16 \text{fruit} + 0.22 \text{mobile\&amp;IT} + 0.42 \text{other}
\end{aligned}\]

<p>This decomposition captures possible scenarios where the word apple appears in a context: which can be a fruit, or a sweet dessert (apple pie!), or of course your favorite iphone. The remaining unexplained component may contain other contextualized information.</p>

<p>If you are familiar with <a href="https://aclanthology.org/N13-1090.pdf">word embedding</a>, similar phenomenon is also observed there: “King” - “Queen” \(\approx\) “Man” - “Woman”. Broadly speaking, this can be viewed as a form of factor models in statistics.</p>

<h2 id="how-is-composition-represented-inside-a-transformer">How is composition represented inside a Transformer?</h2>

<p>LRH helps us to understand the representation of concepts in Transformers. But what about compositions? To find out how compositions are represented, let’s consider a simple <strong>copying task</strong>:</p>

\[\begin{aligned}
  \ldots [A], [B], [C] \ldots [A], [B] \quad \xrightarrow{\text{next-token prediction}} 
  \ldots [A], [B], [C]\ldots [A], [B],{[C]}.
\end{aligned}\]

<p>Here \([A], [B], [C]\) are three words (or tokens) in a prompt where irrelevant words may sit between the patterns. The goal is for the model to copy the repetition pattern.</p>

<p>Tasks of similar nature have been studied in the <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction head</a> literature.</p>
<blockquote>
  <p>An induction head is an attention head in a Transformer with certain copying-related behavior.</p>
</blockquote>

<p>Let me summarize a few interesting phnenomena about induction heads.</p>
<ul>
  <li>Induction heads can be used to solve copying, which requires at least two self-attention layers.</li>
  <li>Once induction heads appear in a Transformer, they can generalize on OOD data</li>
  <li>Induction heads are important to the emergence of ICL and accompany phase transitions in training dynamics</li>
</ul>

<p>In our experiment, we trained a two-layer Transformer on synthetic sequences that contain repetition patterns. We evaluated the model on two test data, one with the same distribution as the \(P_{\mathrm{train}}\)(in-distribution or ID) and another on a different distribution where we changed the token distribution and increased the pattern length (OOD). The figure is what we got.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/main_high_res.png" alt="Phase transition on copying" /></p>

<p>In the training dynamics, the model visibly experiences two phases.</p>
<ul>
  <li>Weak learning: the model bases its prediction on the marginal distribution of tokens. It helps in-distribution accuracy a bit, but does not help and OOD accuracy</li>
  <li>Rule learning: the model figures out the copying rule by forming the induction head, thus achieving low ID/OOD errors.
We also found that two-layer Transformer, in comparison, did not learn how to solve copying.</li>
</ul>

<h4 id="subspace-matching">Subspace matching.</h4>
<p>How do we explain this sudden emergence of induction heads and OOD generalization? (“Why does the model suddenly get smarter?”) We’ve taken many other measurements when training the Transformer. Roughly speaking, our findings can be described as follows.</p>

<blockquote>
  <p>The first self-attention layer outputs intermediate vectors in a subspace, and the second self-attention layer reads vectors from in another subspace. The two subspaces <strong>becomes suddenly aligned</strong> at the transition.</p>
</blockquote>

<p>This provides an explanation for how compositional structures are formed within a model.</p>

<h2 id="from-lrh-to-common-bridge-subspace-hypothesis">From LRH to Common Bridge Subspace Hypothesis</h2>

<p>This simple experiment motivates us to generalize LRH to representation of compositions.</p>

<blockquote>
  <p>For compositional tasks, a latent subspace stores intermediate representations from the outputs of relevant attention heads and then matches later heads.</p>
</blockquote>

<p style="text-align: center;"><img src="/assets/images/induction-head/CBR-2.png" alt="Illustration of Common Bridge Subspace Hypothesis" width="900" /></p>

<p>To be more clear, each self-attention layer has a weight matrix \(W_{\mathrm{OV}}\) responsible for “writing” the information, and a weight matrix \(W_{\mathrm{QK}}\) responsible for “reading” the information. This is in fact the <a href="https://transformer-circuits.pub/2021/framework/index.html">intuition</a> behind many mechanistic interpretability papers. If you are familiar with how Transformers work, then the following formula may help you understand the <strong>circuits</strong> perspective.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/transformers-circuits.png" alt="Circuits view of a Transformer" width="500" /></p>

<p>Mathematically, the <strong>common bridge subspace hypothesis</strong> can be stated as</p>

\[\begin{equation}
V = \mathrm{span}(W_{\mathrm{OV}}) = \mathrm{span}(W_{\mathrm{QK}})
\end{equation}\]

<p>for selected OV and QK weight matrices. The subspace \(V\) can be viewed a “bridge subspace” that connects two layers in order to represent compositions.</p>

<p>We’ve tested this hypothesis with projection-based ablation studies on many LLMs, and it appears to hold broadly. If you are thoughts on this hypothesis, dropping me a message is extremely welcome!</p>

<h2 id="induction-heads-are-critical-components-across-llms-and-tasks">Induction heads are critical components across LLMs and tasks</h2>

<p>A lot of our analyses center on induction heads, because they are so pervasive and important! They help us gain a lot of understanding about composition, OOD generalization, and geometric insights such as the common bridge subspace hypothesis.</p>

<p>We’ve confirmed prior findings about induction heads on a variety of LLMs (10+ models, ranging from 36M to 70B) on several tasks:</p>
<ul>
  <li>Fuzzy copying</li>
  <li>Indirect object identification</li>
  <li>In-context learning</li>
  <li>Math reasoning with chain-of-thought on GSM8K</li>
</ul>

<p>We’ve conducted ablation experiments where we removed induction heads from the model one by one and then measured the accuracy. Below are some representative plots.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/summary-example-no-cot.png" alt="Induction heads are crucial for OOD generalization" /></p>

<p>So removal of induction heads leads to a significant drop of accuracy on symbolic prompts (recall the first figure), but not on the normal prompts. This result paints a likely picture about how LLMs solve language tasks.</p>

<blockquote>
  <p>LLMs rely on memorized facts for ID prompts, and use combined abilities
(both memorized facts and IHs) to solve OOD prompts.</p>
</blockquote>

<h2 id="final-thoughts">Final thoughts</h2>

<p>Our study is far from complete, but I am excited that many interesting phenomena have emerged. It is worth thinking about a number of questions.</p>

<ul>
  <li>Interpreting feature representations with better decomposition.
 SAEs have been successful in explaining what models “believe” at a given layer. However, many questions remain unclear to me, such as
    <ul>
      <li>how concepts are composed across layers,</li>
      <li>how models solve complicated reasoning tasks, and</li>
      <li>how the rules are represented by models
Perhaps we can identify more structures that do not represent base concepts but are functionally important. The observation of bridge subspaces may be useful.</li>
    </ul>

    <p style="text-align: center;"><img src="/assets/images/induction-head/takeaway-2.png" alt="Representations of concepts vs. rules" width="400" /></p>
  </li>
  <li>Memorization and generalization.
It is still not clear to me how models pick up and memorize concepts, and use some of the memorized concepts selectively in a context. Especially for reasoning models such as OpenAI’s o1 and DeepSeek-R1. It’s definitely more costly if I am going to run complicated reasoning experiments. Hopefully I can get some help this year :-)</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is based on my recent paper published in PNAS. If you can’t get past the paywall, here is a arXiv version. Big thanks to my collaborator Jiajun Song and Zhuoyan Xu!]]></summary></entry><entry><title type="html">Hidden Geometry of Large Language Models</title><link href="http://localhost:4000/jekyll/update/2023/10/28/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Hidden Geometry of Large Language Models" /><published>2023-10-28T22:00:00-05:00</published><updated>2023-10-28T22:00:00-05:00</updated><id>http://localhost:4000/jekyll/update/2023/10/28/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/10/28/welcome-to-jekyll.html"><![CDATA[<p>This post is based on a <a href="https://arxiv.org/abs/2310.04861">paper</a> with <a href="https://github.com/JiajunSong629">Jiajun Song</a>.</p>

<h2 id="can-we-trust-large-language-models">Can we trust Large Language Models?</h2>
<p>If you are reading this blog, most probably you know what <a href="https://openai.com/chatgpt">ChatGPT</a> is. “<em>How fascinating!</em>” You may exclaim. Yes, it is indeed magical to interact with a chatbot that seemingly understand our languages. The more we play with ChatGPT, the more surprises we have: it is super knowledgeable, good at drafting emails, polishing essays, writing codes, and so on!</p>

<p>But then wait … when you give elementary-level mathematical questions—as simple as multiplication—as prompts, it sometimes outputs wrong solutions. There are many pitfuls: LLMs often <a href="https://apnews.com/article/artificial-intelligence-hallucination-chatbots-chatgpt-falsehoods-ac4672c5b06e6f91050aa46ee731bcf4">hallucinate</a>, and they cannot infer <a href="https://arxiv.org/pdf/2309.12288.pdf">basic relations</a> such as “A is B” implying “B is A” . So we ask—</p>

<ul>
  <li>Can we trust ChatGPT as well as other Large Lauguage Models (LLMs)?</li>
  <li>Is it possible that LLMs contain harmful information or bias?</li>
  <li>Shouldn’t we worry that LLMs may distort information and influence people’s opinons?</li>
</ul>

<p>Indeed, if our future society depends heavily on an advancing technology, there are good reasons to be concerned about the worst case scenarios (think about industrial revoluation and global warming).</p>

<p>There are now heated <a href="https://www.youtube.com/watch?v=byYlC2cagLw">discussions</a> about ChatGPT. AI leaders such as Yoshua Bengio, Geoffrey Hinton, Andrew Yao are calling for safe AI systems in a recent <a href="https://managing-ai-risks.com/">post</a>.</p>

<h2 id="towards-mechanistic-explanation">Towards mechanistic explanation</h2>

<h3 id="reflecting-on-history-emergence-of-black-box-models">Reflecting on history: Emergence of black-box models</h3>
<p>When I first took courses in data analysis as an undergrad about a decade ago, every method is based on principled generative models: assuming that our data follow a linear model with observational noise, then we know how to extract signals from data, often in an <em>optimal</em> way. Moreover, we reasonably believe that data usually possess nice <em>geometric structures</em>, such as <strong>smoothness</strong> and <strong>sparsity</strong>. Principled mathematical analysis leads to ubiquitous methods such as <a href="https://www.jstor.org/stable/2346178">LASSO</a> and <a href="https://ieeexplore.ieee.org/document/1614066">compressed sensing</a>.</p>

<p>However, when data are very complex—think about images, audios, texts—we are limited by our abilities to model the data precisely. An early neural network model, <a href="https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">AlexNet</a>, shows that large models with multiple layers are very efficient at capturing nonlinearity. This marks the beginning of the deep learning revolutionary.</p>

<p>Neural networks as black-box models are highly accurate, yet much less interpretable. <strong>Lack of interpretability</strong> is a persistent and crucial issue in LLMs, as people build larger and larger models based on modules we don’t quite understand.</p>

<h3 id="visualizing-attention-matrix">Visualizing attention matrix</h3>

<p>Since the <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper, <strong>Transformers</strong> become the de facto building blocks of LLMs. Sometimes we can peep inside the model by plotting the <strong>attention matrices</strong>, which suggest how information of words in a sequence is combined. Mathematical speaking, given embeddings (or hidden states) \(\boldsymbol{x}_1 \ldots \boldsymbol{x}_T \in \mathbb{R}^d\), we stack them into a matrix \(\boldsymbol{X} = [\boldsymbol{x}_1, \ldots, \boldsymbol{x}_T]^\top \in \mathbb{R}^{T \times d}\) and compute a \(T \times T\) attention matrix</p>

\[\begin{equation}

\boldsymbol{A} = \mathrm{softmax}\left(\frac{\boldsymbol{X} \boldsymbol{W}^q (\boldsymbol{W}^k)^\top \boldsymbol{X}^\top}{\sqrt{d_{\mathrm{head}}}} \right)\, . \tag{1} \label{attn}

\end{equation}\]

<p>Here, \(d_{\mathrm{head}}\) is some dimension, \(\boldsymbol{W}^q\) and \(\boldsymbol{W}^k\) are trained weights in transformers, and \(\mathrm{softmax}\) applies the standard softmax</p>

\[\begin{aligned}
 \left( \mathrm{softmax}(\boldsymbol{z}) \right)_{k} := \frac{e^{z_k}}{\sum_{j} e^{z_j}}, \qquad \text{where}~\boldsymbol{z} \in \mathbb{R}^T 
\end{aligned}\]

<p>to every row vector of the matrix inside the paranthesis. For a given input sequence, visualizing the attention matrices (there are many in a transformer!) helps us to understand how information is processed. For example, I pass some random sequence to <a href="https://huggingface.co/docs/transformers/model_doc/gpt2">GPT-2</a>, and examine the associated attention matrix \(\boldsymbol{A}\) from layer 5, head 1. It is worthwhile to note that many GPT-style transformers (including chatGPT!) are based on <strong>next-token prediction</strong>, so each token is only allowed to attend to past tokens in a sequence. Below I can use separate heatmaps (left) to visualize three selected attention matrices (Layer 0, Head 1; Layer 2, Head 2; Layer 5, Head 1), and overlapping three bipartite graphs (right) to visualize the same three matrices.</p>

<p><img src="/assets/images/Blog-attn-drawio.png" alt="Three attention heads\label{attn}" /></p>

<p>On the right side of the bipartite graph, each token is connected to tokens on the left. The thicker a line is, the more information we assemble to form the hidden states in the next layer. For example, the red lines (based on one attention matrix) mean that a token prefers to focus on the previous token.</p>

<h3 id="interpretability-some-recent-progress">Interpretability: some recent progress</h3>

<p><a href="https://www.anthropic.com/">Anthropic</a> pinoneers <strong>Mechanistic Interpretability</strong> of neural networks, in which weights and activations are examined and analyzed closely. By studying transformers and extensive experiments, they identified that an interesting component, called the induction head, exists universally in popular trained transformered</p>

<p><a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"><strong>Induction Head</strong></a> is a circuit—often a collection of several attention heads—within a transformer than functions the <em>copying</em> mechanism: given a sequence that contains tokens [A], [B] … [A], an induction head outputs values that leads to predicting [B]. In other words, induction heads complete an observed pattern by looking at the previous tokens in a sequence.</p>

<p>A surprising phenomenon is that induction heads function as copying mechanisms even when [A], [B] are drawn from distributions that are totally different from training data! In Figure \ref{attn}, we already saw an exmaple of induction head: once observing [A], [B] … [A], the attention heads focus heavily on the adjacent token [B] of the previous identical token [A]. Note that the input sequence is a repetition of independent random tokens, yet GPT-2 is trained on natural languages.</p>

<div style="text-align: center;">
  <img src="/assets/images/induction-head.gif" alt="Attention visualization for 3 attnetion heads simultaneously" width="350" />
</div>
<!--![Attention visualization for 3 attnetion heads simultaneously](/assets/images/induction-head.gif)-->

<p>The above GIF highlights what each token attends to in the three attention heads as in Figure \ref{attn}.</p>

<p>Analyzing abstract abilities like induction heads in trained transformers opens the door to understanding <a href="https://en.wikipedia.org/wiki/Prompt_engineering">in-context learning</a> and other emergent abilities of LLMs. The more we understand, the better we are at reining in LLMs before large AI sysmtems wreak havoc!</p>

<h2 id="from-heuristics-to-uncovering-hidden-geometry">From heuristics to uncovering hidden geometry</h2>

<p>When people build new neural network architectures or devise new optimization tricks, they often rely on heuristics which vaguely describe the information some activations or weights contain. For example, transformers consist of many <em>self-attention layers</em>, each of which is believed to <strong>contextualize</strong> features progressively, i.e., combining information within contexts to form high-level features.</p>

<p>If we examine the arguments people make in the literature, an implicit and recurring assumption seems to be the following:</p>

<p style="text-align: center;">
<i>Hidden states (or activations) in intermediate layers of transformers carry certain information about</i> <b>input sequence contexts</b>, <i>and certain information about</i> <b>token positions</b>.
</p>

<p>Can we understand more precisely what information hidden states contain?</p>

<h3 id="anova-inspired-decomposition">ANOVA-inspired decomposition</h3>

<p>Suppose that we are given an input sequence, where each token has an <em>embedding</em>, that is, representation by numerical values \(\boldsymbol{h_t}^{(0)} \in \mathbb{R}^d\) where \(t\) is an integer indicating the position of the token. Let us write each layer of a transformer as \(\mathrm{TFLayer}_\ell\), which maps a sequence of hidden states to another for a given initial embeddings \(\boldsymbol{h}_1^{(\ell)}, \ldots, \boldsymbol{h}_T^{(\ell)} \in \mathbb{R}^d\):</p>

\[\begin{aligned}
  \boldsymbol{h}_1^{(\ell+1)}, \ldots, \boldsymbol{h}_T^{(\ell+1)} \leftarrow \mathrm{TFLayer}_\ell \left( \boldsymbol{h}_1^{(\ell)}, \ldots, \boldsymbol{h}_T^{(\ell)} \right)
\end{aligned}\]

<p>Let’s sample as many sequences as we want, feed each sequence through a trained transformer, and collect all these hidden states (or intermediate-layer embeddings) as an array \(\boldsymbol{h}^(\ell) \in \mathbb{R}^{C \times T \times d}\). Here \(C\) is the number of sequences we sample, \(T\) is the sequence length, and \(d\) is the dimension of hidden states.</p>

<p>A classical idea in statistics, known as <a href="https://en.wikipedia.org/wiki/Two-way_analysis_of_variance">ANOVA</a>, is to study the <em>mean effects</em> of each factor in a collection of observations. Based on \(\boldsymbol{h}^{(\ell)}\), we can calculate the mean vectors for every position and for every input sequence.</p>

\[\begin{aligned}
  \boldsymbol{pos}_t^{(\ell)} := \frac{1}{C} \sum_{c=1}^C \boldsymbol{h}_{t,c}^{(\ell)} - \boldsymbol{\mu}^{(\ell)} \in \mathbb{R}^d , \qquad \boldsymbol{ctx}_c^{(\ell)} := \frac{1}{T} \sum_{t=1}^T \boldsymbol{h}_{t,c}^{(\ell)} - \boldsymbol{\mu}^{(\ell)}  \in\mathbb{R}^d
\end{aligned}\]

<p>where \(\boldsymbol{\mu}^{(\ell)} := \frac{1}{CT} \sum_{c,t} \boldsymbol{h}_{t,c}^{(\ell)}\) is the global mean vector. Using these mean vectors, we can decompose hidden states into interpretable components \(\boldsymbol{pos}_t^{(\ell)}\) and \(\boldsymbol{ctx}_c^{(\ell)}\), which we will call <strong>positional vectors</strong> and <strong>context vectors</strong>.</p>

<h3 id="discovering-hidden-geometry">Discovering hidden geometry</h3>

<p><ins> <b>Finding 1.</b> </ins> What does the vectors \(\boldsymbol{pos}_1^{(\ell)}, \ldots \boldsymbol{pos}_T^{(\ell)}\) look like? For each fixed \(\ell\), we can perform <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (or PCA), which projects multidimensional vectors onto a 2D plane. Using standard GPT-2 as the trained transformer to compute mean vectors, we apply PCA and get the following plots.</p>

<p><img src="/assets/images/PCA_none_new.png" alt="PCA visualization of positional mean vectors in each layer" /></p>

<p>Each one of the blue points—which look like a curve but are formed by individual points—correspond to a positional vector. Each red point correspond to \(\boldsymbol{h}_{c,t}^{(\ell)} - \boldsymbol{pos}_t^{(\ell)}\), namely component unexplained by the positional vectors. Light colors mean the start of a sequence (small \(t\)) and dark colors mean the end of a sequence (large \(t\)).</p>

<p>So we just discovered something visably nice: positional vectors form a continuous and spiral shape. Moreover, we can use a form of <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">spectral analysis</a> and <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">Fourier analysis</a> to  show that they lie in an approximately <strong>low-dimensional subspace</strong>, and are mostly <strong>low-frequency</strong> signals! To sum up, this means</p>

<p style="text-align: center;">
<i>Within hidden states, positional information resides in a low-dimensional subspace and forms low-frequency signals.</i>
</p>

<p>Some extensive experiments suggest that this geometric structure is consistent across layers, transformer models, and datasets.</p>

<p><ins> <b>Finding 2.</b> </ins> What about context vectors? If we draw input sequences from various documents, it is natural to expect sequences from the similar documents to have similar contexts. Indeed, this is the idea behind the classical <a href="https://en.wikipedia.org/wiki/Topic_model">topic models</a>.</p>

<p>Let’s find out. First, we normalize the positional vectors and context vectors:</p>

\[\begin{aligned}
  \boldsymbol{P} = \Big[ \frac{\mathbf{pos}_1}{\| \mathbf{pos}_1\|}, \ldots, \frac{\mathbf{pos}_T}{\| \mathbf{pos}_T\|} \Big] , \qquad  \boldsymbol{C} = \Big[ \frac{\mathbf{ctx}_1}{\| \mathbf{ctx}_1\|}, \ldots, \frac{\mathbf{ctx}_C}{\| \mathbf{ctx}_C\|} \Big]
\end{aligned}\]

<p>and then calculate the Gram matrix \(\boldsymbol{G} = [\boldsymbol{P}, \boldsymbol{C}]^\top [\boldsymbol{P}, \boldsymbol{C}]\) of size \((T+C) \times (T+C)\).</p>

<p><img src="/assets/images/gram_openwebtext_topic_gpt2.png" alt="Gram matrix" /></p>

<p>In each of the 12 plots, we feed input sequences sampled from 4 documents into GPT-2, extract hidden states \(\boldsymbol{h}^{(\ell)}\), and then calculate the Gram matrix for each layer. The block structure on the bottom right of the Gram matrix indicates <strong>cluster</strong> structure of context vectors.</p>

<p style="text-align: center;">
<i>Context information across different input sequences form cluster structures, which depend on sources sequences are sampled from.</i>
</p>

<p><ins> <b>Finding 3.</b> </ins> Now we look at the cross interaction between positional vectors and context vectors. The cosine similarities (namely inner products between normalized vectors) are showns in the top right part of the Gram matrices. The values are close to zero, which means the positional vectors and the context vectors are <strong>nearly orthogonal</strong>! This geometric structure is known as <a href="https://en.wikipedia.org/wiki/Mutual_coherence_(linear_algebra)">mutual incoherence</a> in the classical literature of compressed sensing, dictionary learning, low-rank matrix recovery, and so on.</p>

<p>Incoherence structure is known to be “algorithm-friendly”, since it generally makes recovering complementary bases much easier. Perhaps this is why training neural networks with stochastic gradient descent can capture complex associations in texts easily. While I do not have a complete theory, preliminary analysis does suggest the following.</p>

<p style="text-align: center;">
<i>With incoherence structure, interactions between positional vectors and context vectors are more flexible.</i>
</p>

<h2 id="smoothness-key-structure-in-natural-languages">Smoothness: key structure in natural languages</h2>

<p>It seems that we have made progress toward decipher the mechanism of transformers. <em>But do we understand natural languages better?</em> Natural languages as text-based data are used for training transformers? After all, if we had trained transformers with pure random data, we shouldn’t expect meaning structure at all!</p>

<p>In Finding 1, we identified a low-rank and low-frequency positional structure. It is well understood that this structure is intimately connected to the inherent smoothness of data. Following this philosophy, we provide some analysis and implication in our <a href="https://arxiv.org/abs/2310.04861">paper</a>, which is summarized by the following links.</p>

<p><img src="/assets/images/link.png" alt="Link" /></p>

<p>What is the picture without smoothness? Consider a very simple arithemetic tasks: the input sequence is “a+b=c” where “a” and “b” are digits of length between 5 and 10. The following shows an example of the input sequence</p>

<p style="text-align: center;">
'6985972+2780004021=2786989993'
</p>

<p>The correct solution “c” is obviously sensitive to positions of individual digits; if we shift any digit by 1, generally we get completely wrong solution.</p>

<h3 id="failure-of-length-generalization">Failure of length generalization</h3>

<p>By adapting an implementation of GPT-style model <a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>, we are able to train a smaller scale Transformer for the addition problem. By drawing suffiicient training data, the validation error drops to zero. But when we sample  “a” and “b” with a different length—smaller or larger number of digits—the trained transformer performs disastrously! This is a typical failure of <a href="https://arxiv.org/pdf/2207.04901.pdf">length generalization</a> (or extrapolation).</p>

<p>Let us examine what is going wrong in our trained transformer. We focus on two attention heads: Layer 0 Head 1, and Layer 2 Head 3. In the plots below, each row shows the position-position Gram matrix, the QK matrix (namely matrix before applying softmax in Eqn. \ref{attn}), and the attention matrix.</p>

<p><img src="/assets/images/Blog-smoothness-drawio.png" alt="Smoothness" /></p>

<p><strong>Discontinuity</strong> is a visible structure in our transformer trained on addition inputs. If digits are shortened, lengthened, or shifted in the test data, it will be hard for the model to locate the positions correctly! It seems reasonable to</p>

<p style="text-align: center;">
<i>For math-related tasks, instability issues may arise from discontinuity patterns inside a transformer, making generalization and extrapolation fail.</i>
</p>

<h2 id="final-thoughts">Final thoughts</h2>

<p>Many LLMs are being developed and employed in applications every month. No doubt, interpretability-enhanced LLMs would make our society much safer. While lots of things need to be done, recent research does provide hope for more interpretable models. Here are some questions I think are interesting to explore.</p>

<ol>
  <li>How does geometry in transformers increase interpretability? Can we devise informative visualization methods and reliable measurements to monitor what is happening inside black-box models?</li>
  <li>Can we characterize the smoothness of training data more precisely? If we observe discontinuity patterns, how do we overcome the shortcoming of current transformer models?</li>
</ol>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is based on a paper with Jiajun Song.]]></summary></entry></feed>