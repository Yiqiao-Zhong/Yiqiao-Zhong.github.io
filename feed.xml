<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-02-08T22:47:58-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Advancing Interpretability of Deep Learning</title><subtitle>Can we understand the inner workings of black-box models? The goal of the blogs is to explore structures and analyze empirical phenoemna by scientific experiments on deep learning. </subtitle><entry><title type="html">Shattered compositionality: how transformers learn arithmetic rules</title><link href="http://localhost:4000/jekyll/update/2026/02/08/shattered-compositionality.html" rel="alternate" type="text/html" title="Shattered compositionality: how transformers learn arithmetic rules" /><published>2026-02-08T18:00:00-06:00</published><updated>2026-02-08T18:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2026/02/08/shattered-compositionality</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2026/02/08/shattered-compositionality.html"><![CDATA[<p>I’ve had a recent <a href="https://arxiv.org/abs/2601.22510">paper</a> with students where we made quite intriguing findings on transformers’ learning dynamics.</p>

<p><strong>A lurking nightmare</strong>. Given the undisputed performance of recent LLMs such as GPT-5 and Claude Code, many of us have forgotten the early days of ChatGPT when it frequently made <a href="https://www.reddit.com/r/ChatGPT/comments/13nm89a/when_i_ask_chatgpt_multiply_the_following_numbers/">errors</a> in basic arithmetic such as multiplication. Yet, the training recipe for pretraining, which is <strong>autoregressive training</strong> (aka next-token prediction) on transformers, stays largely the same. What has caused persistent defects in the current models will likely pass on to future successors. Of course, better training data and tool use have reduced hallucination for now, but who knows what awaits us when this technology is applied to many new domains on a large scale? A recent <a href="https://www.nature.com/articles/s41586-025-09937-5">Nature paper</a> on emergent misalignment shows the disconcerting risks.</p>

<p>A lingering puzzle of LLMs remains unsolved—</p>

<h2 id="how-do-models-build-their-own-rules">How do models build their own rules?</h2>

<p>Here is a recurring question that has fueled endless debates since three years ago:</p>

<p><em>Are LLMs rule learners or pattern matching machines?</em></p>

<p>AI proponents contend that LLMs are capable of any knowledge work, while skeptics argue that LLMs are merely stochastic parrots that mix texts from training data. The gap between these two opposing narratives does not narrow if the lingering puzzle remains unsolved—how do these models figure out rules from data that differ from human’s expectation? While AI alignment develops practical techniques to address this mismatch, we have poor understanding about <em>what</em> causes the mismatch and <em>how</em> it emerges from autoregressive training.</p>

<h2 id="dissecting-learning-dynamics-on-synthetic-experiments">Dissecting learning dynamics on synthetic experiments</h2>

<p>Now that it has become impossible for most people to pretrain an LLM, we decided to train a small transformer from scratch on synthetic data. There are two big advantages of such synthetic experiments: first, we can track the model’s learning behavior using comprehensive evaluation metrics; second, we have full understanding and control over the training data. The synthetic experiments are not the end point of our study; instead they are means to a bigger end: understanding how learning dynamics induces a model’s peculiar behaviors. Of course, this doesn’t capture everything about real LLMs—but it lets us isolate one mechanism very cleanly.</p>

<p>One of the simple arithmetic tasks we studied in the paper is to solve a 4-operand addition. Our training data are text strings of the format $a + b + c + d = e$ where \(a,b,c,d\) are integers sampled uniformly from 0 to 999 and \(e\) is the correct result. Each string is a training example, tokenized character-by-character as one input sequence to the transformer. We adapted a simple training <a href="https://github.com/karpathy/nanoGPT">implementation</a> to our setting. Since the model scale is very small, each complete run of the training cost little on Google Colab.</p>

<p>What do we expect from autoregressive training on our synthetic data? Let’s consider an example:</p>

\[\begin{equation}
(\text{plain format})~~~~~~349 + 102 + 382 + 907 = 1740.
\end{equation}\]

<p>When a model is given the context (characters) preceding the first output digit 1, it needs to figure out the thousands-place digit by using all hundreds-place digits (namely 3, 1, 3, 9) before the equality, together with possible carry from lower digit places. Next, in order to correctly predict the hundreds-place digit 7 of the output digit, the model needs to compute a modulus-10 calculation together with possible carry from lower digit places. But this awkward calculation order is not what we learn in elementary school! It is much easier for start with units, the lowest digit place.</p>

<p>Let’s consider a human-friendly format where we reverse the order in the output integer.</p>

\[\begin{equation}
(\text{reverse format})~~~~~~349 + 102 + 382 + 907 = 0471.
\end{equation}\]

<p>Now, the first digit in the output integer is 0, which is the modulus-10 result of \(9+2+2+7\). We don’t need to worry about carry as it is the unit place. Next, predicting the tens-place \(4\) requires calculating the modulus-10 result of \(4+0+8+0\) and carry from the unit place. By this point, we usually have kept our calculation for the unit place in our scratch paper, so it is easy to proceed with this order.</p>

<p>There are reasons to believe that the reverse format is easier to learn not only for humans but also for models, as advocated by this <a href="https://arxiv.org/abs/2307.03381">paper</a>, because learning individual digits amounts to a composition of modulus-10 operations and carry in a linear progression.</p>

<h2 id="the-puzzle-of-reverse-learning-order">The puzzle of reverse learning order</h2>

<p>This expectation proves to be wrong. After running experiments separately with both output formats, we’ve found that transformers “insist” on learning from the thousands-place digit regardless of the output format!</p>

<p style="text-align: center;"><img src="/assets/images/shattered-compositionality/addition-learning-order.png" alt="Addition-learning-order" width="800" /></p>

<p>The digit-wise error rates (comparing the model’s prediction with the correct answer) in the figure indicate that there is still a persistent linear learning order, but in the opposite direction to human’s rules. The setting with reverse output format that is supposed to be easier to learn, actually causes the model to learn slower: convergence takes much longer in the right plot, and the plateaus in the evolution of error rates suggest undesired loss landscapes.</p>

<p>As we became baffled by this puzzle, an examination of the error distribution showed another striking phenemenon.</p>

<p style="text-align: center;"><img src="/assets/images/shattered-compositionality/gaussian-errors.png" alt="Error-distribution" width="800" /></p>

<p>We computed the difference between the correct integer and the model’s predicted integer \(\hat{e} - e\) and treated this error as a whole instead of four separate digits. The figure shows that the errors are approximately Gaussian distributed, and the spread narrows as training proceeds. It tells us that the model behaves like an approximation algorithm—first figuring out the rough range of the output integer with a wide error distribution, then gradually refining the prediction with narrowing error distribution. Rather than learning the underlying algebraic rules of addition, the model appears to solve the task using an approximate, analytical strategy.</p>

<p>Can scaling fix this non-human learning order? We’ve tested the addition task with larger models and conducted a finetuning experiment using Pythia-1B. All experiments showed the consistent reverse learning order. We came to the conclusion that transformers are not rule learners, unless being fed with carefully designed reasoning data.</p>

<h2 id="correlational-matching-drives-learning-dynamics">Correlational matching drives learning dynamics</h2>

<p>What is the cause of the counterintuitive learning dynamics? We turned to pattern matching as a plausible explanation. But a deeper understanding requires a definition of what we mean by the “pattern” or “signal” the model learns throughout training. I was motivated by the mutual information (MI) metrics in an earlier <a href="https://arxiv.org/abs/2305.18654">paper</a>, so we considered the following metrics as a measurement of correlational structure from training data (recall that MI is zero if two random variables are independent). Let \(a=a_1a_2a_3\) be the digit representation of first input integer and \(e=e_0e_1e_2e_3\) be the output integer. According to our sampling of training data, all digits \(a_i\) and $e_j$$ can be viewed as random variables.</p>

<ul>
  <li>Mutual information \(I(a_1; e_0)\): how much information the hundreds-place digit \(a_1\) provides for predicting the thousands-place digit \(e_0\)</li>
  <li>Conditional mutual information \(I(a_i; e_i \mid c_{i-1})\): how much information the input digit provides for predicting the output digit at the same digit place, <em>conditional</em> on the carry to the higher digit.</li>
</ul>

<p>A positive MI means there is <strong>learnable signal</strong> in the training data, which can be captured by the (stochastic) gradient descent as we train a model. For example, a large \(a_1\) is more likely to yield a large \(e_0\), so this correlational structure offers a signal for the model to catch. Interestingly, \(I(a_i; e_i)\) itself is zero, whereas conditioning on the carry at a higher digit \(c_{i-1}\) yields positive MI \(I(a_i; e_i \mid c_{i-1})\). This provides a plausibility for the reverse learning order—a model starts with the signal in \(I(a_1; e_0)\) and proceed to lower digits since conditioning on acquired ``skills’’ at higher digits provides learnable signals.</p>

<p style="text-align: center;"><img src="/assets/images/shattered-compositionality/mutual-information.png" alt="Mutual-information" width="600" /></p>

<p>We devised similar MI metrics to measure a model’s predictive behavior throughout training, and compared the evoluation of a model’s MI metrics against those calculated from the training data. The figure shows that critical drops in training loss match closely with phase transitions in the mutual information metrics. As we examined these plots, it became clear that correlational matching offers a much more convincing explanation for the model’s learning behavior—learning is driven less by discovering explicit rules and more by exploiting whatever statistical dependencies are easiest to capture early on.</p>

<h2 id="shattered-compositionality-causes-brittle-performance">Shattered compositionality causes brittle performance</h2>

<p>For arithmetic tasks, pattern matching appears to be a winning narrative—but what are the consequences of this unexpected learning order? In our experiments, the errors eventually go down to approximately zero if we training long enough.</p>

<p>We continued our investigation into several other synthetic tasks and expanded to a few LLM experiments as well. We identified a few additional “warning messages”:</p>

<ul>
  <li><strong>Unpredictable learning behavior.</strong> Besides the reverse learning order, a transformer may learn arithmetic in multiple orders concurrently. This parallel learning mechanism often results in “skill competition”, where learning one pattern interferes negatively with learning another.</li>
  <li><strong>Unintended mixing errors.</strong> A model generates unexpected mixing errors where a part of model’s output is repeated or swapped with another part.</li>
  <li><strong>Deteriorating performance under distribution shift</strong>. Correlational matching soon becomes a liability when the test examples are from a different distribution. Even across many high-performing LLMs, slight twists of reasoning benchmarks yield much worse accuracy.</li>
</ul>

<p>Once learning is driven by correlations rather than explicit rules, compositional structure becomes fragile, and failures begin to follow recognizable patterns.</p>

<div class="note">
<strong>Shattered compositionality:</strong> Parallel or reversed skill learning yields a fragile composition, where tensions between skill acquisition can trigger mixing errors under distribution shift.
</div>

<h2 id="final-thoughts">Final thoughts</h2>

<p>Our paper echoes some recent findings on compositionality in LLMs.</p>

<ul>
  <li><em>Parallel mechanisms.</em> In the Anthropic’s <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">blog</a> on the biology of an LLM, they discover the existence of multiple pathways within the model for solving complex tasks, some of which are in synergy and some of which are in competition.</li>
  <li><em>Bag of Heuristics.</em> A recent <a href="https://arxiv.org/abs/2410.21272">paper</a> that argues that LLMs solve compositional tasks with a bag of heuristics instead of using coherent rules.</li>
</ul>

<p>To improve the robustness and transparency of LLMs, we need more evaluation metrics. Standard benchmarking is a good idea, but we can’t predict how LLMs may perform on out-of-distribution data or in a new domain. Our analysis of synthetic tasks is a first step toward building more robust and safer AI systems in the future.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I’ve had a recent paper with students where we made quite intriguing findings on transformers’ learning dynamics.]]></summary></entry><entry><title type="html">Do you interpret your t-SNE and UMAP visualization correctly?</title><link href="http://localhost:4000/jekyll/update/2025/06/01/tsne-visualization.html" rel="alternate" type="text/html" title="Do you interpret your t-SNE and UMAP visualization correctly?" /><published>2025-06-01T22:00:00-05:00</published><updated>2025-06-01T22:00:00-05:00</updated><id>http://localhost:4000/jekyll/update/2025/06/01/tsne-visualization</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/06/01/tsne-visualization.html"><![CDATA[<p>This post is based on a recent <a href="https://rdcu.be/eoG8L">paper</a> published in <em>Nature Communications</em>. Great work by my PhD student <a href="https://github.com/zhexuandliu/MapContinuity-NE-Reliability">Zhexuan Liu</a>!</p>

<h2 id="a-routine-for-visualizing-high-dimensional-data">A routine for visualizing high-dimensional data</h2>

<p>A basic task in data science is visualization of high-dimensional data: given \(n\) data points in high dimensions, how can we get a quick sense of the structure of the data? Apart from <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> which projects data linearly to a subspace, it has become quite common to use t-SNE or UMAP, which reduce data dimensionality <em>nonlinearly</em> to 2D or 3D for visualization.</p>

<p>Let me take the following example from the original <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">t-SNE paper</a>. The <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset consists of images of handwritten digits 0 through 9, each at a resolution of 28×28 pixels. The t-SNE visualization suggests cluster structure for the 10 classes.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/tsne_hinton.png" alt="t-SNE visualization of images from MNIST dataset" width="300" /></p>

<p>A closely related visualization method is <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection">UMAP</a>, which is quite popular in  <a href="https://www.nature.com/articles/nbt.4314">single-cell data analysis</a>, where we want to visualize measurements (high-dimensional points) of a collection of cells in a 2D plane. The following example is taken from <a href="https://www.nature.com/articles/nbt.4314">a paper</a> that studies cell types from UMAP visualization.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/umap-single-cell-2.png" alt="UMAP visualization in single-cell analysis" width="500" /></p>

<h2 id="interpretability-crisis">Interpretability crisis</h2>

<p>It looks amazing, you may say. What’s going wrong in the visualization? Well, t-SNE and UMAP map high-dimensional points nonlinearly to 2D or 3D by design, but the algorithms underlying the visualization methods are highly complicated. A natural question is how faithful t-SNE and UMAP are as nonlinear dimensionality reduction methods.</p>

<ul>
  <li><strong>Q1:</strong> Does large distance between points imply high dissimilarity?</li>
  <li><strong>Q2:</strong> Does clear seperation between clusters imply distinctness of grouping?</li>
</ul>

<p>Perhaps unsurprisingly, many researchers have examined <em>distortion</em> effects of t-SNE and UMAP and gave a negative answer to Q1. Yet, a <strong>continuous</strong> map should at least preserve the <strong>topological</strong> structure of input data—in particular, non-overlapping sets should be still non-overlapping after mapping. Some of the distortion-related issues are addressed in Rong Ma’s prior papers [<a href="https://www.nature.com/articles/s43588-022-00380-4">1</a>, <a href="https://www.nature.com/articles/s41467-023-36492-2">2</a>] and analyzed in [<a href="https://www.jmlr.org/papers/v23/21-0524.html">3</a>].</p>

<p>Human eyes often take for granted the continuity of embedding maps (which is quite reasonable), but an affirmative answer to Q2 is unfounded. <em>Nature</em> reports a <a href="https://www.nature.com/articles/d41586-024-00568-w">controversy</a> centering on the interpretation of a UMAP visulization of genetic data. The following figure which I took from the paper in question appears to show clear separation between different racial groups.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/all-of-us.png" alt="UMAP controversy" width="400" /></p>

<p><em>Is such distinctness a misleading artifact created by UMAP?</em></p>

<h2 id="are-t-sne--umap-manifold-learning-methods">Are t-SNE &amp; UMAP manifold learning methods?</h2>

<p>Tracing the root of this interpretational confusion, I found an official <a href="https://scikit-learn.org/stable/modules/manifold.html">tutorial webpage</a> on manifold learning, which includes t-SNE as one of the manifold learning methods.</p>

<p>Let me explain why this interpretation is <strong>wrong</strong>. Let’s simulate a few hundreds points from a mixture of Gaussian distributions in 2D.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/2GMM.png" alt="Simulation" width="350" /></p>

<p>Then, we run t-SNE and get two typical visualization plots (which one we get depends on the hyperparameters).</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/2GMM-tsne.png" alt="Simulation-tsne" width="500" /></p>

<p>Clearly, there are two types of discontinuity patterns—the one on the left is global, while the one on the right is local. We can see how these discontinuities may lead to unfaithful interpretations.</p>

<ul>
  <li>Left: <strong>Overconfidence-inducing (OI) discontinuity</strong> promotes distinctness of the two overlapping Gaussian distributions</li>
  <li>Right: <strong>Fracture-inducing (FI) discontinuity</strong> creates many spurious sub-clusters inside each Gaussian component.</li>
</ul>

<h2 id="leave-one-out-map-reveals-intrinsic-discontinuities">Leave-one-out map reveals intrinsic discontinuities</h2>

<p>How can we detect the discontinuities in t-SNE and UMAP visualization? To do this, we need to understand how t-SNE and UMAP work. Both methods receive high-dimensional input data \(\boldsymbol{X} = (\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n)\) and return low-dimensional embedding points \(\boldsymbol{Y} = (\boldsymbol{y}_1, \boldsymbol{y}_2, \ldots, \boldsymbol{y}_n)\) by solving an optimization problem that looks like the following.</p>

\[\begin{equation}
\min_{\boldsymbol{y}_1, \ldots, \boldsymbol{y}_n \in \mathbb{R}^2} \sum_{1\le i&lt;j\le n} \ell(w(\boldsymbol{y}_i, \boldsymbol{y}_j); v_{i,j}(\boldsymbol{X})) + Z(\boldsymbol{Y}).
\end{equation}\]

<p>While I am not showing you details here, we can see that the first term captures pairwise interaction between each pair of input points. So every time I perturb a single point, we need to re-run t-SNE or UMAP again to get new embedding points. How annoying! This issue is one reason why the mechanism of t-SNE/UMAP is difficult to understand.</p>

<p>To make our lives easier, let’s assume that adding (or deleting/modifying) a single input point does not change embedding points significantly, which is used extensively in leave-ont-out (LOO) statistical analysis. If we make this LOO assumption (empirically verified in our paper), then we can define a map \(\boldsymbol{f}\) for <em>any</em> input point \(\boldsymbol{x}\):</p>

\[\begin{equation}
\boldsymbol{f}: \boldsymbol{x} \mapsto \mathrm{argmin}_{\boldsymbol{y}} L(\boldsymbol{y}; \boldsymbol{x})
\end{equation}\]

<p>where \(L\), which we call <strong>LOO-loss</strong>, is the partial loss (involving much fewer terms!) derived from the original loss in t-SNE or UMAP:</p>

\[\begin{equation}
L(\boldsymbol{y}; \boldsymbol{x}) = \sum_{1\le i \le n} \ell\Big(w(\boldsymbol{y}_i, \boldsymbol{y}); v_{i,n+1}\big(\begin{bmatrix}\boldsymbol{X} \\  \boldsymbol{x}^\top \end{bmatrix}\big)\Big) 
+ Z\Big(\begin{bmatrix}\boldsymbol{Y} \\ \boldsymbol{y}^\top \end{bmatrix}\Big)  \; .
\end{equation}\]

<p>The loss landscape of \(L\) in terms of \(\boldsymbol{y}\) for a given input point \(\boldsymbol{x}\) gives us a lot of information about the quality of the embedded point. For example, in the previous simulation, when we add an interpolated input point between two Gaussian centers at a constant speed, the embedding point can jump from one cluster to the other cluster abruptly due to the saddle structure of the loss landscape. This means a large topological change of the embedding point \(\boldsymbol{y}\) under a small perturbation to \(\boldsymbol{x}\).</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/discontinuity_50.gif" alt="Loss landscape shows large topological change" width="400" /></p>

<p>Similarly, under a different hyperparamter for t-SNE, the embeddings of interpolated input points reveal an <em>uneven</em> trajactory as we move the interpolated \(\boldsymbol{x}\) at an <em>even</em> speed. This suggests that embedding points are prone to getting trapped in many local minima—which is a cause of spurious sub-cluster we saw earlier!</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/discontinuity_5.gif" alt="Loss landscape shows many local minima" width="400" /></p>

<h2 id="diagnosing-unreliable-embedding-points">Diagnosing unreliable embedding points</h2>

<p>We’ve seen the potential existence of unreliable embedding points, such as intermixing points (mixture of two classes) getting “sucked” into two clusters. To combat this issue, we devised two types of diagnosis scores based on perturbing input points in our LOO-loss function. Calculation of both types of scores works as a wrapper without requiring ground-truth labels. Let me illustrate how the first type of diagnosis scores look like.</p>

<p>Let’s sample a few thousands images from CIFAR-10, and consider applying a standard neural network feature extract such as ResNet-18 to obtain image feature vectors. We run t-SNE and obtain the visualization as follows.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/cifar10-tsne.png" alt="t-SNE visualization of CIFAR-10 image features" width="400" /></p>

<p>Since we are not sure if the separation between the clusters is an artifict, we run our algorithm and obtain diagnostic scores shown below.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/cifar10-diagnosis.png" alt="t-SNE diagnosis of CIFAR-10 image features" width="400" /></p>

<p>The yellow-ish points indicate that those embedding points are likely to be unreliable. Indeed, when we further examined the original images of those points, we found that they look quite ambiguous in terms of the class membership (namely highly uncertain to classify), yet t-SNE misleadingly embeds those image features to cluster boundaries. Without the diagnostic scores, we would probably have a <strong>reduced perception of uncertainty</strong>.</p>

<h2 id="concluding-remarks-how-to-trust-visualization">Concluding remarks: how to trust visualization?</h2>

<p>Here are some lessons we’ve learned.</p>

<ul>
  <li>Stop thinking of t-SNE and UMAP as manifold learning methods.</li>
  <li>As with other visualization tools, use and interpret t-SNE and UMAP with caution.</li>
  <li>Try different visualization tools or diagnosis if possible.</li>
</ul>

<p>Interpreting visualization results inevitably involves some degree of subjectivity, and ultimately, it is up to the user to strike the right balance between expediency and rigor in scientific investigation.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is based on a recent paper published in Nature Communications. Great work by my PhD student Zhexuan Liu!]]></summary></entry><entry><title type="html">Imbalance troubles: Why is the minority class hurt more by overfitting?</title><link href="http://localhost:4000/jekyll/update/2025/03/31/imbalanced-classification.html" rel="alternate" type="text/html" title="Imbalance troubles: Why is the minority class hurt more by overfitting?" /><published>2025-03-31T22:00:00-05:00</published><updated>2025-03-31T22:00:00-05:00</updated><id>http://localhost:4000/jekyll/update/2025/03/31/imbalanced-classification</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/03/31/imbalanced-classification.html"><![CDATA[<p>This post is based on a recent <a href="https://arxiv.org/abs/2502.11323">paper</a>.</p>

<h2 id="prelude-overfitting-in-neural-networks">Prelude: Overfitting in neural networks</h2>

<p>Deep neural networks such as ResNets and Transformers are standard recipes for classification. 
They are powerful feature extractors that learn feature representations of data as vectors in high dimensions. They have a tendancy to fit data well—often achieving 100% train accuracy.
In fact, a well-known <a href="https://arxiv.org/abs/1611.03530">paper</a> shows that popular CNN models can be easily trained to fit corrupted labels (a fraction of labels are arbitrarily permuted).
This paper was a bit shocking because of a deep-rooted belief in statistics: a model can’t memorize and generalize well at the same time (<a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias-variance tradeoff</a>).</p>

<p>Since then, a lot of research activities have focused on understanding why neural networks generalize well on test data despite perfect fits. 
The notion of <a href="https://www.pnas.org/doi/10.1073/pnas.1907378117"><strong>benign overfitting</strong></a> and subsequent research seem to have largely resolved the puzzle.
The key idea is that when the model has a large number of parameters (thus achieving the perfect train accuracy), then there is an implicit regularization effect that encourages the model to be parsimonious.
In the simple case of linear classification, when training data is linearly separable, then <a href="https://jmlr.org/papers/v19/18-188.html"><strong>max-margin classifier</strong></a> is the “preferred” classifier.</p>

<h2 id="overfitting-headache-in-imbalanced-classification">Overfitting headache in imbalanced classification</h2>

<p class="image-caption"><img src="/assets/images/imbalanced-classification/imbalance-intro.png" alt="A common pipeline for classification tasks" /></p>

<p>Many datasets are highly imbalanced, which means that there are minority classes whose training samples are much fewer than the other classes (majority classes).
Data imbalance is especially common in downstream analysis. For example, 
doctors want to classify medical images by leveraging existing models; but images with diseases (say cancer) are often scarce.</p>

<p>A common practice is to take a pretrained neural network (for example, ResNets or vision Transformers) and use it as a feature extractor. 
For each image, we will have a feature vector \(\boldsymbol{x}_i\) using the neural network. 
If we have labels \(y_i\) (\(i=1,2,\ldots,n\)) for the images on a downstream task, then we can simply train a linear classifier such as logistic regression or support vector machine (SVM) using the dataset \((\boldsymbol{x}_i, y_i)\).
(A more sophisticated approach is to finetune the last few layers of the nerual net, but we won’t discuss this here.)</p>

<p>Suppose that we only have two classes (\(y_i \in \{\pm 1\}\)) and the training data is linear separable, namely, 
there exists a vector \(\boldsymbol{\beta}\) such that \(\boldsymbol{\beta}^\top x_i &gt; 0\) if and only if \(y_i=1\). 
The <a href="https://jmlr.org/papers/v19/18-188.html">implicit bias</a> of gradient descent algorithms is known to promote the max-margin classifier, which can be obtained by including a tiny \(\ell_2\) regularization or running the gradient descent for sufficiently long time.
After fitting a max-margin classifier, we obtain the coefficients \((\boldsymbol{\beta}, \beta_0)\) where \(\beta_0\) is the intercept term.</p>

<p><img src="/assets/images/imbalanced-classification/ELD.png" alt="Logit distribution on train dataset (histogram) vs. test dataset (dashed curve)" /></p>

<p>The <strong>logit</strong> usually refers to the real-valued scalar before we apply Softmax to get prediction probabilities.
In our case, for a feature \(\boldsymbol{x}\), the logit is \(\boldsymbol{\beta}^\top \boldsymbol{x} + \beta_0\). 
In the figure above, we generated features from a mixture of two Gaussians, and fit a max-margin classifier.
Then we plotted the distribution of logits on the training dataset as histograms which are fitted by solid curves, 
and the distribution of logits on the test dataset fitted by dashed curves. Two colors indicate the two classes. 
We’ve also found similar results across <strong>various data modalities</strong>, including tabular data, image data, and text data.</p>

<p>What do we discover? The logit distributions are clearly different on train vs. test datasets. More concretely,</p>

<ul>
  <li>The logit distributions on the training dataset look like <strong>truncated Gaussian</strong> distributions;</li>
  <li>The minority logit distribution (right cluster, histogram rescaled for better visibility) is “eaten” more by the truncation, thus having worse accuracy.</li>
</ul>

<p>Hmmm, why is there a bigger discrepancy for the minority class in terms of logit distributions between the training dataset vs. the test dataset?
A branch of statistical theory, <em>high-dimensional statistics</em>, offers perfect tools for understanding such phenomenon.</p>

<h2 id="a-tour-of-recent-statistical-theory-trouble-of-high-dimensionality">A tour of recent statistical theory: trouble of high dimensionality</h2>

<p>In statistics, a classifier obtained from a training dataset is viewed as inherently random, because the training examples are random samples from the universe of all possible examples (known as population);
Think about drawing a different training dataset, then the classifier should also be different. 
But fortunately, for linear classifiers, classical statistical theory tells us that the variability is small if the feature dimension \(d\) is much smaller than the sample size \(n\).
For our previous experiment, we would expect small when \(d \ll n\).</p>

<ul>
  <li>Consistency: \(\boldsymbol{\beta}\) is close to the “true” coefficient vector \(\boldsymbol{\beta}\) if we had infinite training samples.</li>
  <li>Normal distribution: the logit distribution is a projection of multivariate features into 1D, which is univariate Gaussian (recall that linear projection of multivariate Gaussian is still <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation">Gaussian</a>).</li>
</ul>

<p>However, this classical picture falls apart if \(d\) and \(n\) are comparable. When the dimension is getting large, there is too much flexibility (known as degree of freedom) for a classifier to fit the training data, 
so it gets more “uncertain”. Roughly speaking, this is the source of overfitting when the dimension is too large or the model has a higher fitting capability.</p>

<p>It turns out that there is a <strong>phase transition</strong> phenomenon: there exists a critical threshold for \(d/n\), 
above which the training data is linearly separable, and the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> (MLE) is not well defined.
See <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-1/The-phase-transition-for-the-existence-of-the-maximum-likelihood/10.1214/18-AOS1789.full">this paper</a> for example.</p>

<p>To understand intuitively why dimensionality matters for classification problems, consider a simple case where \(n=d\) and each feature is one of the canonical basis, 
that is, the \(i\)-th coordinate of \(\boldsymbol{x}_i\) is 1 and other coordinates are 0. Then for any positive scaler \(c\), 
the coefficient vector \(\sum_{i=1}^n \big[ c \mathbf{1}\{y_i=1\} - \mathbf{1}\{y_i=-1\} \big] \boldsymbol{x}_i\) perfectly separates the two classes, no matter what labels \(y_i\) are.
This simple case is actually not that artificial, 
as \(n\) points in general position in a \(d\)-dimensional space with \(d \ge n\) can be mapped to the canonical basis by an affine transformation.</p>

<h2 id="rethinking-overfitting-through-logit-distribution">Rethinking overfitting through logit distribution</h2>

<p>All is not lost when we are in very high dimensions. 
As we saw earlier, the max-margin linear classifier—which is unique and has often generalizes well—is the “preferred” classifier thanks to implicit regularization. 
But overfitting does create an issue for the minority class.</p>

<p>Our theory finds that the distortion (truncation) of logit distributions <em>completely explains</em> overfitting. The following heuristic explanations are derived from our theory.</p>

<ol>
  <li>High dimensionality allows arbitrary distortion of logit distributions up to a certain limit (measured by the Wasserstein distance);</li>
  <li>Since the minority class weights less and counts less toward the limit, its logit distribution is more severely distorted in order to maximize the margin;</li>
  <li>Large distortion of the minority logit distribution leads to worse test accuracy, despite perfect train accuracy.</li>
</ol>

<p>Some of the intuitions (especially the first point) and technical tools already appear in prior work such as <a href="https://arxiv.org/abs/2206.06526">this paper</a> on projection pursuit.</p>

<h2 id="consequence-and-margin-rebalancing">Consequence and margin rebalancing</h2>

<p>Our analysis reveals that data imbalance not only exacerbates test accuray for the minority class, but increases errors for <a href="https://arxiv.org/pdf/1706.04599">calibration</a> as well.
The general trend we’ve found can be summarized by the following.</p>
<blockquote>
  <p>In terms of impact on test accuracy and calibration, degree of data imbalance \(\approx\) noise level.</p>
</blockquote>

<p>What can we do to counter overfitting for the minority class? 
In many applications such as medical image classification, we want the minority class to be correctly classified as much as the majority class.
We studied a common strategy where margins are rebalanced based on the sample sizes of both classes.</p>

<p><img src="/assets/images/imbalanced-classification/SVM-cartoon1.png" alt="Margin rebalancing shifts the decision boundary and improves minority accuracy" /></p>

<p>In some situations, we discovered that margins rebalancing is indispensable: 
the balanced test error is as worse as a random guess without margin rebalancing, but is close to zero with appropriate margin rebalancing.</p>

<h2 id="epilogue-why-it-matters">Epilogue: why it matters</h2>

<p>Okay, you might say, it is an interesting phenomenon but how it is relevant to machine learning practice? 
I don’t have a conclusive answer. Arguably our current analysis of linear classifiers is a bit simpler—
but it is potentially connected to a wide range of practice in deep learning (feedback is welcome!).</p>

<ul>
  <li>Pretraining: margin-aware adjustment in the loss function design</li>
  <li>Fine-tuning: distribution shifts and bias correction</li>
  <li>Interpretability: linear probing of features or network activations</li>
</ul>

<p>Let me give some concrete examples. In computer vision, <a href="https://papers.nips.cc/paper_files/paper/2019/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html">researchers</a> have used similar intuitions to rebalance margins in designing loss functions for pretraining.
When adapting and interpreting language models, <a href="https://arxiv.org/abs/2306.03341">linear probing</a> is widely used to distinguish desired features and harmful features.</p>

<p>It is quite conceivable that linear classifiers are building blocks of existing or future AI systems, as models need to represent concepts as clusters in the feature space.
For improving AI safety, we’d better understand how bias is generated from training. Analyzing linear classifiers is probably just a starting point!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is based on a recent paper.]]></summary></entry><entry><title type="html">Can LLMs solve novel tasks? Induction heads, composition, and out-of-distribution generalization</title><link href="http://localhost:4000/jekyll/update/2025/02/18/induction-heads.html" rel="alternate" type="text/html" title="Can LLMs solve novel tasks? Induction heads, composition, and out-of-distribution generalization" /><published>2025-02-18T21:00:00-06:00</published><updated>2025-02-18T21:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2025/02/18/induction-heads</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/18/induction-heads.html"><![CDATA[<p>This post is based on my recent <a href="https://www.pnas.org/doi/10.1073/pnas.2417182122">paper</a> published in PNAS. If you can’t get past the paywall, here is a <a href="https://arxiv.org/abs/2408.09503">arXiv version</a>.
Big thanks to my collaborator <a href="https://github.com/JiajunSong629">Jiajun Song</a> and <a href="https://pages.cs.wisc.edu/~zxu444/home/">Zhuoyan Xu</a>!</p>

<h2 id="can-llms-solve-novelty-tasks-the-agi-debate">Can LLMs solve novelty tasks? The AGI debate</h2>

<p>Since ChatGPT, there have been a lot of polarizing opinons about LLMs. One focal point is the novelty of the generated texts by an LLM.</p>

<p>Proponents point to the examples of creative solutions produced by ChatGPT and Claude, hailing these models as AGI or superhuman intelligence. Others argue that these models are simply parroting internet text and incapable of generating truly original generation.</p>

<p>Well…it all depends on how you define novelty and creativity, in my opinon. In statistics, a standard and well-studied situation is to train a model on data drawn from the training distribution \(P_{\mathrm{train}}\) and evaluate it on the same test distribution \(P_{\mathrm{test}}\). 
This classical notation of generalization requires training a model for each specific task. As many researchers know, GPT-3 and ChatGPT are smarter than that—which is epitomized by the <a href="https://arxiv.org/abs/2005.14165"><strong>in-context learning</strong></a> (ICL).</p>

<p>Here is a simple proposal to study “novelty” of models, which is often called out-of-distribution (OOD) generalization:
\(\begin{aligned}
P_{\mathrm{train}} \neq P_{\mathrm{test}}.
\end{aligned}\)</p>

<h2 id="llms-generalize-ood-on-certain-reasoning-tasks">LLMs generalize OOD on certain reasoning tasks</h2>

<p>Let’s say someone asks you to play a game: given a prompt</p>
<blockquote>
  <p>“Then, <em>Henry</em> and <em>Blake</em> had a long argument. Afterwards <em>Henry</em> said to”</p>
</blockquote>

<p>what is the next probable word? It is easy to guess Blake as the natural continuation. Now what if the prompt is changed a bit?</p>
<blockquote>
  <p>“Then, \&amp;\^ and #$ had a long argument. Afterwards \&amp;\^ said to”</p>
</blockquote>

<p>It may take you some time to realize that I simply replaced the names with special symbols. Once seeing this point, you probably say #$.</p>

<p>Another example of symbolic replacement is a variant of <strong>ICL</strong>:</p>
<blockquote>
  <p>baseball is $#, celery is !\%, sheep is \&amp;*, volleyball is $#, lettuce is</p>
</blockquote>

<p>This prompt asks you to classifying objects into three classes, which are represented by…well…strange symbols. If you follow the replacement logic, then you will probably guess the natural continuation !\%.</p>

<p>While LLMs (especially earlier versions) are unlikely to have seen these strange artificially modified sentences during pretraining, we do see a point in these examples. Training data are mixed with natural languages, math expression, code, and other formatted text such as html. Testing models with perturbed prompts help us understand what models can do and cannot do.</p>

<p>A simple experiment shows how some of the open-source LLMs perform on these two tasks, with and without symbol replacement.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/table1.png" alt="A first look at OOD generalization" /></p>

<p>So, do LLMs solve novel tasks? I’d say yes, because the models seem to have figured out (not perfectly but definitely better than random guess) the “replacement rule” even when they are not explicitly trained on the above specific examples. Instances of the similar nature are observed in the literature, promoting the <a href="https://arxiv.org/abs/2303.12712">“Spark of AGI”</a> argument.</p>

<h2 id="representation-of-concepts-and-linear-representation-hypothesis-lrh">Representation of concepts and linear representation hypothesis (LRH)</h2>

<p>Can we undestand the model’s mechanism a bit better? In the past two years, a lot of interpretability analyses focus on the representation of concepts within a Transformer, most notably the <strong>sparse autoencoder</strong> (SAE) papers, one from <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/">Anthropic</a> and another from <a href="https://cdn.openai.com/papers/sparse-autoencoders.pdf">OpenAI</a> last year. These papers examine the embeddings (or hidden states) of a prompt, which is a simply a vector in the Euclidean space. 
My <a href="https://yiqiao-zhong.github.io/jekyll/update/2023/10/28/welcome-to-jekyll.html">previous blog</a> also explores the geometry of embeddings across network layers.</p>

<p>A key observation is the following:</p>
<blockquote>
  <p>Concepts are represented as linear subspaces in the embedding space.</p>
</blockquote>

<p>Let me give you a concrete example. Suppose that I have a prompt that contains the word “apple” as the last word. Now if I pass the prompt into an LLM, how does the embedding vector looks like?</p>

<p>Representing a word or a prompt as a linear combination of base concepts is an important intution for explaining the inner workings of LLMs. It is often referred to as the <a href="https://arxiv.org/abs/2311.03658"><strong>linear representation hypothesis</strong></a> (LRH).</p>

\[\begin{aligned}
\text{apple} = 0.09 \times \text{dessert} + 0.11 \times \text{organism} + 0.16 \times \text{fruit} + 0.22  \text{mobile\&amp;IT} + 0.42 \times \text{other}
\end{aligned}\]

<p>This decomposition captures possible scenarios where the word apple appears in a context: it sometimes mean a fruit, or a sweet dessert (apple pie!), or of course your favorite iphone. The remaining unexplained component may contain other contextualized information.</p>

<p>If you are familiar with <a href="https://aclanthology.org/N13-1090.pdf">word embedding</a>, similar phenomenon is also observed there: “King” - “Queen” \(\approx\) “Man” - “Woman”. Broadly speaking, this can be viewed as a form of factor models in statistics.</p>

<h2 id="how-is-composition-represented-inside-a-transformer">How is composition represented inside a Transformer?</h2>

<p>The LRH helps us to understand the representation of concepts in Transformers. But what about compositions? To find out how compositions are represented, let’s consider a simple <strong>copying task</strong>:</p>

\[\begin{aligned}
  \ldots [A], [B], [C] \ldots [A], [B] \quad \xrightarrow{\text{next-token prediction}} 
  \ldots [A], [B], [C]\ldots [A], [B],{[C]}.
\end{aligned}\]

<p>Here \([A], [B], [C]\) are three words (or tokens) in a prompt where irrelevant words may sit between the patterns. The goal is for the model to copy the repetition pattern.</p>

<p>Tasks of similar nature have been studied in the <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction head</a> literature.</p>
<blockquote>
  <p>An induction head is an attention head in a Transformer with certain copying-related behavior.</p>
</blockquote>

<p>Let me summarize a few interesting phnenomena about induction heads.</p>
<ul>
  <li>Induction heads can be used to solve copying, which requires at least two self-attention layers.</li>
  <li>Once induction heads appear in a Transformer, they can generalize on OOD data.</li>
  <li>Induction heads are important to the emergence of ICL and accompany phase transitions in training dynamics.</li>
</ul>

<p>In our experiment, we trained a two-layer Transformer on synthetic sequences that contain repetition patterns. We evaluated the model on two test data, one with the same distribution as the \(P_{\mathrm{train}}\)(in-distribution or ID) and another on a different distribution where we changed the token distribution and increased the pattern length (OOD). 
The following figure is what we got.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/main_high_res.png" alt="Phase transition on copying" /></p>

<p>In the training dynamics, the model visibly experiences two phases.</p>
<ul>
  <li>Weak learning: the model bases its prediction on the marginal distribution of tokens. It helps in-distribution accuracy a bit, but does not help and OOD accuracy.</li>
  <li>Rule learning: the model figures out the copying rule by forming the induction head, thus achieving low ID/OOD errors.
We also found that two-layer Transformer, in comparison, did not learn how to solve copying.</li>
</ul>

<h4 id="subspace-matching">Subspace matching</h4>
<p>How do we explain this sudden emergence of induction heads and OOD generalization? (“Why does the model suddenly get smarter?”) We’ve taken many other measurements when training the Transformer. Roughly speaking, our findings can be described as follows.</p>

<blockquote>
  <p>The first self-attention layer outputs intermediate vectors in a subspace, and the second self-attention layer reads vectors from in another subspace. The two subspaces <strong>becomes suddenly aligned</strong> at the transition.</p>
</blockquote>

<p>This provides an explanation for how compositional structures are formed within a model.</p>

<h2 id="from-lrh-to-common-bridge-subspace-hypothesis">From LRH to Common Bridge Subspace Hypothesis</h2>

<p>This simple experiment motivates us to generalize LRH to representation of compositions.</p>

<blockquote>
  <p>For compositional tasks, a latent subspace stores intermediate representations from the outputs of relevant attention heads and then matches later heads.</p>
</blockquote>

<p style="text-align: center;"><img src="/assets/images/induction-head/CBR-2.png" alt="Illustration of Common Bridge Subspace Hypothesis" width="900" /></p>

<p>To be more clear, each self-attention layer has a weight matrix \(W_{\mathrm{OV}}\) responsible for “writing” the information, and a weight matrix \(W_{\mathrm{QK}}\) responsible for “reading” the information. This is in fact the <a href="https://transformer-circuits.pub/2021/framework/index.html">intuition</a> behind many mechanistic interpretability papers. If you are familiar with how Transformers work, then the following formula may help you understand the <strong>circuits</strong> perspective.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/transformers-circuits.png" alt="Circuits view of a Transformer" width="500" /></p>

<p>Mathematically, the <strong>common bridge subspace hypothesis</strong> can be stated as</p>

\[\begin{equation}
V = \mathrm{span}(W_{\mathrm{OV}}) = \mathrm{span}(W_{\mathrm{QK}})
\end{equation}\]

<p>for selected OV and QK weight matrices. The subspace \(V\) can be viewed a “bridge subspace” that connects two layers in order to represent compositions.</p>

<p>We’ve tested this hypothesis with projection-based ablation studies on many LLMs, and it appears to hold broadly. If you are thoughts on this hypothesis, dropping me a message is extremely welcome!</p>

<h2 id="induction-heads-are-critical-components-across-llms-and-tasks">Induction heads are critical components across LLMs and tasks</h2>

<p>A lot of our analyses center on induction heads, because they are so pervasive and important! They help us gain a lot of understanding about composition, OOD generalization, and geometric insights such as the common bridge subspace hypothesis.</p>

<p>We’ve confirmed prior findings about induction heads on a variety of LLMs (10+ models, ranging from 36M to 70B) on several tasks:</p>
<ul>
  <li>Fuzzy copying</li>
  <li>Indirect object identification</li>
  <li>In-context learning</li>
  <li>Math reasoning with chain-of-thought on GSM8K</li>
</ul>

<p>We’ve conducted ablation experiments where we removed induction heads from the model one by one and then measured the accuracy. Below are some representative plots.</p>

<p style="text-align: center;"><img src="/assets/images/induction-head/summary-example-no-cot.png" alt="Induction heads are crucial for OOD generalization" /></p>

<p>So removal of induction heads leads to a significant drop of accuracy on symbolic prompts (recall the first figure), but not on the normal prompts. This result paints a likely picture about how LLMs solve language tasks.</p>

<blockquote>
  <p>LLMs rely on memorized facts for ID prompts, and use combined abilities
(both memorized facts and IHs) to solve OOD prompts.</p>
</blockquote>

<h2 id="final-thoughts">Final thoughts</h2>

<p>Our study is far from complete, but I am excited that many interesting phenomena have emerged. It is worth thinking about a number of questions.</p>

<ul>
  <li>Interpreting feature representations with better decomposition.
 SAEs have been successful in explaining what models “believe” at a given layer. However, many questions remain unclear to me, such as
    <ul>
      <li>how concepts are composed across layers,</li>
      <li>how models solve complicated reasoning tasks, and</li>
      <li>how the rules are represented by models
Perhaps we can identify more structures that do not represent base concepts but are functionally important. The observation of bridge subspaces may be useful.</li>
    </ul>

    <p style="text-align: center;"><img src="/assets/images/induction-head/takeaway-2.png" alt="Representations of concepts vs. rules" width="400" /></p>
  </li>
  <li>Memorization and generalization.
It is still not clear to me how models pick up and memorize concepts, and use some of the memorized concepts selectively in a context. Especially for reasoning models such as OpenAI’s o1 and DeepSeek-R1. It’s definitely more costly if I am going to run complicated reasoning experiments. Hopefully I can get some help this year :-)</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is based on my recent paper published in PNAS. If you can’t get past the paywall, here is a arXiv version. Big thanks to my collaborator Jiajun Song and Zhuoyan Xu!]]></summary></entry><entry><title type="html">Hidden Geometry of Large Language Models</title><link href="http://localhost:4000/jekyll/update/2023/10/28/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Hidden Geometry of Large Language Models" /><published>2023-10-28T22:00:00-05:00</published><updated>2023-10-28T22:00:00-05:00</updated><id>http://localhost:4000/jekyll/update/2023/10/28/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/10/28/welcome-to-jekyll.html"><![CDATA[<p>This post is based on a <a href="https://arxiv.org/abs/2310.04861">paper</a> with <a href="https://github.com/JiajunSong629">Jiajun Song</a>.</p>

<h2 id="can-we-trust-large-language-models">Can we trust Large Language Models?</h2>
<p>If you are reading this blog, most probably you know what <a href="https://openai.com/chatgpt">ChatGPT</a> is. “<em>How fascinating!</em>” You may exclaim. Yes, it is indeed magical to interact with a chatbot that seemingly understand our languages. The more we play with ChatGPT, the more surprises we have: it is super knowledgeable, good at drafting emails, polishing essays, writing codes, and so on!</p>

<p>But then wait … when you give elementary-level mathematical questions—as simple as multiplication—as prompts, it sometimes outputs wrong solutions. There are many pitfuls: LLMs often <a href="https://apnews.com/article/artificial-intelligence-hallucination-chatbots-chatgpt-falsehoods-ac4672c5b06e6f91050aa46ee731bcf4">hallucinate</a>, and they cannot infer <a href="https://arxiv.org/pdf/2309.12288.pdf">basic relations</a> such as “A is B” implying “B is A” . So we ask—</p>

<ul>
  <li>Can we trust ChatGPT as well as other Large Lauguage Models (LLMs)?</li>
  <li>Is it possible that LLMs contain harmful information or bias?</li>
  <li>Shouldn’t we worry that LLMs may distort information and influence people’s opinons?</li>
</ul>

<p>Indeed, if our future society depends heavily on an advancing technology, there are good reasons to be concerned about the worst case scenarios (think about industrial revoluation and global warming).</p>

<p>There are now heated <a href="https://www.youtube.com/watch?v=byYlC2cagLw">discussions</a> about ChatGPT. AI leaders such as Yoshua Bengio, Geoffrey Hinton, Andrew Yao are calling for safe AI systems in a recent <a href="https://managing-ai-risks.com/">post</a>.</p>

<h2 id="towards-mechanistic-explanation">Towards mechanistic explanation</h2>

<h3 id="reflecting-on-history-emergence-of-black-box-models">Reflecting on history: Emergence of black-box models</h3>
<p>When I first took courses in data analysis as an undergrad about a decade ago, every method is based on principled generative models: assuming that our data follow a linear model with observational noise, then we know how to extract signals from data, often in an <em>optimal</em> way. Moreover, we reasonably believe that data usually possess nice <em>geometric structures</em>, such as <strong>smoothness</strong> and <strong>sparsity</strong>. Principled mathematical analysis leads to ubiquitous methods such as <a href="https://www.jstor.org/stable/2346178">LASSO</a> and <a href="https://ieeexplore.ieee.org/document/1614066">compressed sensing</a>.</p>

<p>However, when data are very complex—think about images, audios, texts—we are limited by our abilities to model the data precisely. An early neural network model, <a href="https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">AlexNet</a>, shows that large models with multiple layers are very efficient at capturing nonlinearity. This marks the beginning of the deep learning revolutionary.</p>

<p>Neural networks as black-box models are highly accurate, yet much less interpretable. <strong>Lack of interpretability</strong> is a persistent and crucial issue in LLMs, as people build larger and larger models based on modules we don’t quite understand.</p>

<h3 id="visualizing-attention-matrix">Visualizing attention matrix</h3>

<p>Since the <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper, <strong>Transformers</strong> become the de facto building blocks of LLMs. Sometimes we can peep inside the model by plotting the <strong>attention matrices</strong>, which suggest how information of words in a sequence is combined. Mathematical speaking, given embeddings (or hidden states) \(\boldsymbol{x}_1 \ldots \boldsymbol{x}_T \in \mathbb{R}^d\), we stack them into a matrix \(\boldsymbol{X} = [\boldsymbol{x}_1, \ldots, \boldsymbol{x}_T]^\top \in \mathbb{R}^{T \times d}\) and compute a \(T \times T\) attention matrix</p>

\[\begin{equation}

\boldsymbol{A} = \mathrm{softmax}\left(\frac{\boldsymbol{X} \boldsymbol{W}^q (\boldsymbol{W}^k)^\top \boldsymbol{X}^\top}{\sqrt{d_{\mathrm{head}}}} \right)\, . \tag{1} \label{attn}

\end{equation}\]

<p>Here, \(d_{\mathrm{head}}\) is some dimension, \(\boldsymbol{W}^q\) and \(\boldsymbol{W}^k\) are trained weights in transformers, and \(\mathrm{softmax}\) applies the standard softmax</p>

\[\begin{aligned}
 \left( \mathrm{softmax}(\boldsymbol{z}) \right)_{k} := \frac{e^{z_k}}{\sum_{j} e^{z_j}}, \qquad \text{where}~\boldsymbol{z} \in \mathbb{R}^T 
\end{aligned}\]

<p>to every row vector of the matrix inside the paranthesis. For a given input sequence, visualizing the attention matrices (there are many in a transformer!) helps us to understand how information is processed. For example, I pass some random sequence to <a href="https://huggingface.co/docs/transformers/model_doc/gpt2">GPT-2</a>, and examine the associated attention matrix \(\boldsymbol{A}\) from layer 5, head 1. It is worthwhile to note that many GPT-style transformers (including chatGPT!) are based on <strong>next-token prediction</strong>, so each token is only allowed to attend to past tokens in a sequence. Below I can use separate heatmaps (left) to visualize three selected attention matrices (Layer 0, Head 1; Layer 2, Head 2; Layer 5, Head 1), and overlapping three bipartite graphs (right) to visualize the same three matrices.</p>

<p><img src="/assets/images/Blog-attn-drawio.png" alt="Three attention heads\label{attn}" /></p>

<p>On the right side of the bipartite graph, each token is connected to tokens on the left. The thicker a line is, the more information we assemble to form the hidden states in the next layer. For example, the red lines (based on one attention matrix) mean that a token prefers to focus on the previous token.</p>

<h3 id="interpretability-some-recent-progress">Interpretability: some recent progress</h3>

<p><a href="https://www.anthropic.com/">Anthropic</a> pinoneers <strong>Mechanistic Interpretability</strong> of neural networks, in which weights and activations are examined and analyzed closely. By studying transformers and extensive experiments, they identified that an interesting component, called the induction head, exists universally in popular trained transformered</p>

<p><a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"><strong>Induction Head</strong></a> is a circuit—often a collection of several attention heads—within a transformer than functions the <em>copying</em> mechanism: given a sequence that contains tokens [A], [B] … [A], an induction head outputs values that leads to predicting [B]. In other words, induction heads complete an observed pattern by looking at the previous tokens in a sequence.</p>

<p>A surprising phenomenon is that induction heads function as copying mechanisms even when [A], [B] are drawn from distributions that are totally different from training data! In Figure \ref{attn}, we already saw an exmaple of induction head: once observing [A], [B] … [A], the attention heads focus heavily on the adjacent token [B] of the previous identical token [A]. Note that the input sequence is a repetition of independent random tokens, yet GPT-2 is trained on natural languages.</p>

<div style="text-align: center;">
  <img src="/assets/images/induction-head.gif" alt="Attention visualization for 3 attnetion heads simultaneously" width="350" />
</div>
<!--![Attention visualization for 3 attnetion heads simultaneously](/assets/images/induction-head.gif)-->

<p>The above GIF highlights what each token attends to in the three attention heads as in Figure \ref{attn}.</p>

<p>Analyzing abstract abilities like induction heads in trained transformers opens the door to understanding <a href="https://en.wikipedia.org/wiki/Prompt_engineering">in-context learning</a> and other emergent abilities of LLMs. The more we understand, the better we are at reining in LLMs before large AI sysmtems wreak havoc!</p>

<h2 id="from-heuristics-to-uncovering-hidden-geometry">From heuristics to uncovering hidden geometry</h2>

<p>When people build new neural network architectures or devise new optimization tricks, they often rely on heuristics which vaguely describe the information some activations or weights contain. For example, transformers consist of many <em>self-attention layers</em>, each of which is believed to <strong>contextualize</strong> features progressively, i.e., combining information within contexts to form high-level features.</p>

<p>If we examine the arguments people make in the literature, an implicit and recurring assumption seems to be the following:</p>

<p style="text-align: center;">
<i>Hidden states (or activations) in intermediate layers of transformers carry certain information about</i> <b>input sequence contexts</b>, <i>and certain information about</i> <b>token positions</b>.
</p>

<p>Can we understand more precisely what information hidden states contain?</p>

<h3 id="anova-inspired-decomposition">ANOVA-inspired decomposition</h3>

<p>Suppose that we are given an input sequence, where each token has an <em>embedding</em>, that is, representation by numerical values \(\boldsymbol{h_t}^{(0)} \in \mathbb{R}^d\) where \(t\) is an integer indicating the position of the token. Let us write each layer of a transformer as \(\mathrm{TFLayer}_\ell\), which maps a sequence of hidden states to another for a given initial embeddings \(\boldsymbol{h}_1^{(\ell)}, \ldots, \boldsymbol{h}_T^{(\ell)} \in \mathbb{R}^d\):</p>

\[\begin{aligned}
  \boldsymbol{h}_1^{(\ell+1)}, \ldots, \boldsymbol{h}_T^{(\ell+1)} \leftarrow \mathrm{TFLayer}_\ell \left( \boldsymbol{h}_1^{(\ell)}, \ldots, \boldsymbol{h}_T^{(\ell)} \right)
\end{aligned}\]

<p>Let’s sample as many sequences as we want, feed each sequence through a trained transformer, and collect all these hidden states (or intermediate-layer embeddings) as an array \(\boldsymbol{h}^(\ell) \in \mathbb{R}^{C \times T \times d}\). Here \(C\) is the number of sequences we sample, \(T\) is the sequence length, and \(d\) is the dimension of hidden states.</p>

<p>A classical idea in statistics, known as <a href="https://en.wikipedia.org/wiki/Two-way_analysis_of_variance">ANOVA</a>, is to study the <em>mean effects</em> of each factor in a collection of observations. Based on \(\boldsymbol{h}^{(\ell)}\), we can calculate the mean vectors for every position and for every input sequence.</p>

\[\begin{aligned}
  \boldsymbol{pos}_t^{(\ell)} := \frac{1}{C} \sum_{c=1}^C \boldsymbol{h}_{t,c}^{(\ell)} - \boldsymbol{\mu}^{(\ell)} \in \mathbb{R}^d , \qquad \boldsymbol{ctx}_c^{(\ell)} := \frac{1}{T} \sum_{t=1}^T \boldsymbol{h}_{t,c}^{(\ell)} - \boldsymbol{\mu}^{(\ell)}  \in\mathbb{R}^d
\end{aligned}\]

<p>where \(\boldsymbol{\mu}^{(\ell)} := \frac{1}{CT} \sum_{c,t} \boldsymbol{h}_{t,c}^{(\ell)}\) is the global mean vector. Using these mean vectors, we can decompose hidden states into interpretable components \(\boldsymbol{pos}_t^{(\ell)}\) and \(\boldsymbol{ctx}_c^{(\ell)}\), which we will call <strong>positional vectors</strong> and <strong>context vectors</strong>.</p>

<h3 id="discovering-hidden-geometry">Discovering hidden geometry</h3>

<p><ins> <b>Finding 1.</b> </ins> What does the vectors \(\boldsymbol{pos}_1^{(\ell)}, \ldots \boldsymbol{pos}_T^{(\ell)}\) look like? For each fixed \(\ell\), we can perform <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (or PCA), which projects multidimensional vectors onto a 2D plane. Using standard GPT-2 as the trained transformer to compute mean vectors, we apply PCA and get the following plots.</p>

<p><img src="/assets/images/PCA_none_new.png" alt="PCA visualization of positional mean vectors in each layer" /></p>

<p>Each one of the blue points—which look like a curve but are formed by individual points—correspond to a positional vector. Each red point correspond to \(\boldsymbol{h}_{c,t}^{(\ell)} - \boldsymbol{pos}_t^{(\ell)}\), namely component unexplained by the positional vectors. Light colors mean the start of a sequence (small \(t\)) and dark colors mean the end of a sequence (large \(t\)).</p>

<p>So we just discovered something visably nice: positional vectors form a continuous and spiral shape. Moreover, we can use a form of <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">spectral analysis</a> and <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">Fourier analysis</a> to  show that they lie in an approximately <strong>low-dimensional subspace</strong>, and are mostly <strong>low-frequency</strong> signals! To sum up, this means</p>

<p style="text-align: center;">
<i>Within hidden states, positional information resides in a low-dimensional subspace and forms low-frequency signals.</i>
</p>

<p>Some extensive experiments suggest that this geometric structure is consistent across layers, transformer models, and datasets.</p>

<p><ins> <b>Finding 2.</b> </ins> What about context vectors? If we draw input sequences from various documents, it is natural to expect sequences from the similar documents to have similar contexts. Indeed, this is the idea behind the classical <a href="https://en.wikipedia.org/wiki/Topic_model">topic models</a>.</p>

<p>Let’s find out. First, we normalize the positional vectors and context vectors:</p>

\[\begin{aligned}
  \boldsymbol{P} = \Big[ \frac{\mathbf{pos}_1}{\| \mathbf{pos}_1\|}, \ldots, \frac{\mathbf{pos}_T}{\| \mathbf{pos}_T\|} \Big] , \qquad  \boldsymbol{C} = \Big[ \frac{\mathbf{ctx}_1}{\| \mathbf{ctx}_1\|}, \ldots, \frac{\mathbf{ctx}_C}{\| \mathbf{ctx}_C\|} \Big]
\end{aligned}\]

<p>and then calculate the Gram matrix \(\boldsymbol{G} = [\boldsymbol{P}, \boldsymbol{C}]^\top [\boldsymbol{P}, \boldsymbol{C}]\) of size \((T+C) \times (T+C)\).</p>

<p><img src="/assets/images/gram_openwebtext_topic_gpt2.png" alt="Gram matrix" /></p>

<p>In each of the 12 plots, we feed input sequences sampled from 4 documents into GPT-2, extract hidden states \(\boldsymbol{h}^{(\ell)}\), and then calculate the Gram matrix for each layer. The block structure on the bottom right of the Gram matrix indicates <strong>cluster</strong> structure of context vectors.</p>

<p style="text-align: center;">
<i>Context information across different input sequences form cluster structures, which depend on sources sequences are sampled from.</i>
</p>

<p><ins> <b>Finding 3.</b> </ins> Now we look at the cross interaction between positional vectors and context vectors. The cosine similarities (namely inner products between normalized vectors) are showns in the top right part of the Gram matrices. The values are close to zero, which means the positional vectors and the context vectors are <strong>nearly orthogonal</strong>! This geometric structure is known as <a href="https://en.wikipedia.org/wiki/Mutual_coherence_(linear_algebra)">mutual incoherence</a> in the classical literature of compressed sensing, dictionary learning, low-rank matrix recovery, and so on.</p>

<p>Incoherence structure is known to be “algorithm-friendly”, since it generally makes recovering complementary bases much easier. Perhaps this is why training neural networks with stochastic gradient descent can capture complex associations in texts easily. While I do not have a complete theory, preliminary analysis does suggest the following.</p>

<p style="text-align: center;">
<i>With incoherence structure, interactions between positional vectors and context vectors are more flexible.</i>
</p>

<h2 id="smoothness-key-structure-in-natural-languages">Smoothness: key structure in natural languages</h2>

<p>It seems that we have made progress toward decipher the mechanism of transformers. <em>But do we understand natural languages better?</em> Natural languages as text-based data are used for training transformers? After all, if we had trained transformers with pure random data, we shouldn’t expect meaning structure at all!</p>

<p>In Finding 1, we identified a low-rank and low-frequency positional structure. It is well understood that this structure is intimately connected to the inherent smoothness of data. Following this philosophy, we provide some analysis and implication in our <a href="https://arxiv.org/abs/2310.04861">paper</a>, which is summarized by the following links.</p>

<p><img src="/assets/images/link.png" alt="Link" /></p>

<p>What is the picture without smoothness? Consider a very simple arithemetic tasks: the input sequence is “a+b=c” where “a” and “b” are digits of length between 5 and 10. The following shows an example of the input sequence</p>

<p style="text-align: center;">
'6985972+2780004021=2786989993'
</p>

<p>The correct solution “c” is obviously sensitive to positions of individual digits; if we shift any digit by 1, generally we get completely wrong solution.</p>

<h3 id="failure-of-length-generalization">Failure of length generalization</h3>

<p>By adapting an implementation of GPT-style model <a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>, we are able to train a smaller scale Transformer for the addition problem. By drawing suffiicient training data, the validation error drops to zero. But when we sample  “a” and “b” with a different length—smaller or larger number of digits—the trained transformer performs disastrously! This is a typical failure of <a href="https://arxiv.org/pdf/2207.04901.pdf">length generalization</a> (or extrapolation).</p>

<p>Let us examine what is going wrong in our trained transformer. We focus on two attention heads: Layer 0 Head 1, and Layer 2 Head 3. In the plots below, each row shows the position-position Gram matrix, the QK matrix (namely matrix before applying softmax in Eqn. \ref{attn}), and the attention matrix.</p>

<p><img src="/assets/images/Blog-smoothness-drawio.png" alt="Smoothness" /></p>

<p><strong>Discontinuity</strong> is a visible structure in our transformer trained on addition inputs. If digits are shortened, lengthened, or shifted in the test data, it will be hard for the model to locate the positions correctly! It seems reasonable to</p>

<p style="text-align: center;">
<i>For math-related tasks, instability issues may arise from discontinuity patterns inside a transformer, making generalization and extrapolation fail.</i>
</p>

<h2 id="final-thoughts">Final thoughts</h2>

<p>Many LLMs are being developed and employed in applications every month. No doubt, interpretability-enhanced LLMs would make our society much safer. While lots of things need to be done, recent research does provide hope for more interpretable models. Here are some questions I think are interesting to explore.</p>

<ol>
  <li>How does geometry in transformers increase interpretability? Can we devise informative visualization methods and reliable measurements to monitor what is happening inside black-box models?</li>
  <li>Can we characterize the smoothness of training data more precisely? If we observe discontinuity patterns, how do we overcome the shortcoming of current transformer models?</li>
</ol>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is based on a paper with Jiajun Song.]]></summary></entry></feed>