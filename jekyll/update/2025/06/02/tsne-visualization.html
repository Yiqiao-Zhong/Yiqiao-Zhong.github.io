<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Do you interpret your t-SNE and UMAP visualization correctly? | Advancing Interpretability of Deep Learning</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Do you interpret your t-SNE and UMAP visualization correctly?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is based on a recent paper published in Nature Communications. Great work by my PhD student Zhexuan Liu!" />
<meta property="og:description" content="This post is based on a recent paper published in Nature Communications. Great work by my PhD student Zhexuan Liu!" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2025/06/02/tsne-visualization.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2025/06/02/tsne-visualization.html" />
<meta property="og:site_name" content="Advancing Interpretability of Deep Learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-02T11:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Do you interpret your t-SNE and UMAP visualization correctly?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-02T11:00:00+08:00","datePublished":"2025-06-02T11:00:00+08:00","description":"This post is based on a recent paper published in Nature Communications. Great work by my PhD student Zhexuan Liu!","headline":"Do you interpret your t-SNE and UMAP visualization correctly?","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2025/06/02/tsne-visualization.html"},"url":"http://localhost:4000/jekyll/update/2025/06/02/tsne-visualization.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Advancing Interpretability of Deep Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Advancing Interpretability of Deep Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About me</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Do you interpret your t-SNE and UMAP visualization correctly?</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-06-02T11:00:00+08:00" itemprop="datePublished">Jun 2, 2025
      </time></p>
  </header>

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <div class="post-content e-content" itemprop="articleBody">
    <p>This post is based on a recent <a href="https://rdcu.be/eoG8L">paper</a> published in <em>Nature Communications</em>. Great work by my PhD student <a href="https://github.com/zhexuandliu/MapContinuity-NE-Reliability">Zhexuan Liu</a>!</p>

<h2 id="a-routine-for-visualizing-high-dimensional-data">A routine for visualizing high-dimensional data</h2>

<p>A basic task in data science is visualization of high-dimensional data: given \(n\) data points in high dimensions, how can we get a quick sense of the structure of the data? Apart from <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> which projects data linearly to a subspace, it has become quite common to use t-SNE or UMAP, which reduce data dimensionality <em>nonlinearly</em> to 2D or 3D for visualization.</p>

<p>Let me take the following example from the original <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">t-SNE paper</a>. The <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset consists of images of handwritten digits 0 through 9, each at a resolution of 28×28 pixels. The t-SNE visualization suggests cluster structure for the 10 classes.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/tsne_hinton.png" alt="t-SNE visualization of images from MNIST dataset" width="300" /></p>

<p>A closely related visualization method is <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection">UMAP</a>, which is quite popular in  <a href="https://www.nature.com/articles/nbt.4314">single-cell data analysis</a>, where we want to visualize measurements (high-dimensional points) of a collection of cells in a 2D plane. The following example is taken from <a href="https://www.nature.com/articles/nbt.4314">a paper</a> that studies cell types from UMAP visualization.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/umap-single-cell-2.png" alt="UMAP visualization in single-cell analysis" width="500" /></p>

<h2 id="interpretability-crisis">Interpretability crisis</h2>

<p>It looks amazing, you may say. What’s going wrong in the visualization? Well, t-SNE and UMAP map high-dimensional points nonlinearly to 2D or 3D by design, but the algorithms underlying the visualization methods are highly complicated. A natural question is how faithful t-SNE and UMAP are as nonlinear dimensionality reduction methods.</p>

<ul>
  <li><strong>Q1:</strong> Does large distance between points imply high dissimilarity?</li>
  <li><strong>Q2:</strong> Does clear seperation between clusters imply distinctness of grouping?</li>
</ul>

<p>Perhaps unsurprisingly, many researchers have examined <em>distortion</em> effects of t-SNE and UMAP and gave a negative answer to Q1. Yet, a <strong>continuous</strong> map should at least preserve the <strong>topological</strong> structure of input data—in particular, non-overlapping sets should be still non-overlapping after mapping.</p>

<p>Human eyes often take for granted the continuity of embedding maps (which is quite reasonable), but an affirmative answer to Q2 is unfounded. <em>Nature</em> reports a <a href="https://www.nature.com/articles/d41586-024-00568-w">controversy</a> centering on the interpretation of a UMAP visulization of genetic data. The following figure which I took from the paper in question appears to show clear separation between different racial groups.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/all-of-us.png" alt="UMAP controversy" width="400" /></p>

<p><em>Is such distinctness a misleading artifact created by UMAP?</em></p>

<h2 id="are-t-sne--umap-manifold-learning-methods">Are t-SNE &amp; UMAP manifold learning methods?</h2>

<p>Tracing the root of this interpretational confusion, I found an official <a href="https://scikit-learn.org/stable/modules/manifold.html">tutorial webpage</a> on manifold learning, which includes t-SNE as one of the manifold learning methods.</p>

<p>Let me explain why this interpretation is <strong>wrong</strong>. Let’s simulate a few hundreds points from a mixture of Gaussian distributions in 2D.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/2GMM.png" alt="Simulation" width="350" /></p>

<p>Then, we run t-SNE and get two typical visualization plots (which one we get depends on the hyperparameters).</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/2GMM-tsne.png" alt="Simulation-tsne" width="500" /></p>

<p>Clearly, there are two types of discontinuity patterns—the one on the left is global, while the one on the right is local. We can see how these discontinuities may lead to unfaithful interpretations.</p>

<ul>
  <li>Left: <strong>Overconfidence-inducing (OI) discontinuity</strong> promotes distinctness of the two overlapping Gaussian distributions</li>
  <li>Right: <strong>Fracture-inducing (FI) discontinuity</strong> creates many spurious sub-clusters inside each Gaussian component.</li>
</ul>

<h2 id="leave-one-out-map-reveals-intrinsic-discontinuities">Leave-one-out map reveals intrinsic discontinuities</h2>

<p>How can we detect the discontinuities in t-SNE and UMAP visualization? To do this, we need to understand how t-SNE and UMAP work. Both methods receive high-dimensional input data \(\boldsymbol{X} = (\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n)\) and return low-dimensional embedding points \(\boldsymbol{Y} = (\boldsymbol{y}_1, \boldsymbol{y}_2, \ldots, \boldsymbol{y}_n)\) by solving an optimization problem that looks like the following.</p>

\[\begin{equation}
\min_{\boldsymbol{y}_1, \ldots, \boldsymbol{y}_n \in \mathbb{R}^2} \sum_{1\le i&lt;j\le n} \ell(w(\boldsymbol{y}_i, \boldsymbol{y}_j); v_{i,j}(\boldsymbol{X})) + Z(\boldsymbol{Y}).
\end{equation}\]

<p>While I am not showing you details here, we can see that the first term captures pairwise interaction between each pair of input points. So every time I perturb a single point, we need to re-run t-SNE or UMAP again to get new embedding points. How annoying! This issue is one reason why the mechanism of t-SNE/UMAP is difficult to understand.</p>

<p>To make our lives easier, let’s assume that adding (or deleting/modifying) a single input point does not change embedding points significantly, which is used extensively in leave-ont-out (LOO) statistical analysis. If we make this LOO assumption (empirically verified in our paper), then we can define a map \(\boldsymbol{f}\) for <em>any</em> input point \(\boldsymbol{x}\):</p>

\[\begin{equation}
\boldsymbol{f}: \boldsymbol{x} \mapsto \mathrm{argmin}_{\boldsymbol{y}} L(\boldsymbol{y}; \boldsymbol{x})
\end{equation}\]

<p>where \(L\), which we call <strong>LOO-loss</strong>, is the partial loss (involving much fewer terms!) derived from the original loss in t-SNE or UMAP:</p>

\[\begin{equation}
L(\boldsymbol{y}; \boldsymbol{x}) = \sum_{1\le i \le n} \ell\Big(w(\boldsymbol{y}_i, \boldsymbol{y}); v_{i,n+1}\big(\begin{bmatrix}\boldsymbol{X} \\  \boldsymbol{x}^\top \end{bmatrix}\big)\Big) 
+ Z\Big(\begin{bmatrix}\boldsymbol{Y} \\ \boldsymbol{y}^\top \end{bmatrix}\Big)  \; .
\end{equation}\]

<p>The loss landscape of \(L\) in terms of \(\boldsymbol{y}\) for a given input point \(\boldsymbol{x}\) gives us a lot of information about the quality of the embedded point. For example, in the previous simulation, when we add an interpolated input point between two Gaussian centers at a constant speed, the embedding point can jump from one cluster to the other cluster abruptly due to the saddle structure of the loss landscape. This means a large topological change of the embedding point \(\boldsymbol{y}\) under a small perturbation to \(\boldsymbol{x}\).</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/discontinuity_50.gif" alt="Loss landscape shows large topological change" width="400" /></p>

<p>Similarly, under a different hyperparamter for t-SNE, the embeddings of interpolated input points reveal an <em>uneven</em> trajactory as we move the interpolated \(\boldsymbol{x}\) at an <em>even</em> speed. This suggests that embedding points are prone to getting trapped in many local minima—which is a cause of spurious sub-cluster we saw earlier!</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/discontinuity_5.gif" alt="Loss landscape shows many local minima" width="400" /></p>

<h2 id="diagnosing-unreliable-embedding-points">Diagnosing unreliable embedding points</h2>

<p>We’ve seen the potential existence of unreliable embedding points, such as intermixing points (mixture of two classes) getting “sucked” into two clusters. To combat this issue, we devised two types of diagnosis scores based on perturbing input points in our LOO-loss function. Calculation of both types of scores works as a wrapper without requiring ground-truth labels. Let me illustrate how the first type of diagnosis scores look like.</p>

<p>Let’s sample a few thousands images from CIFAR-10, and consider applying a standard neural network feature extract such as ResNet-18 to obtain image feature vectors. We run t-SNE and obtain the visualization as follows.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/cifar10-tsne.png" alt="t-SNE visualization of CIFAR-10 image features" width="400" /></p>

<p>Since we are not sure if the separation between the clusters is an artifict, we run our algorithm and obtain diagnostic scores shown below.</p>

<p style="text-align: center;"><img src="/assets/images/tsne-visualization/cifar10-diagnosis.png" alt="t-SNE diagnosis of CIFAR-10 image features" width="400" /></p>

<p>The yellow-ish points indicate that those embedding points are likely to be unreliable. Indeed, when we further examined the original images of those points, we found that they look quite ambiguous in terms of the class membership (namely highly uncertain to classify), yet t-SNE misleadingly embeds those image features to cluster boundaries. Without the diagnostic scores, we would probably have a <strong>reduced perception of uncertainty</strong>.</p>

<h2 id="concluding-remarks-how-to-trust-visualization">Concluding remarks: how to trust visualization?</h2>

<p>Here are some lessons we’ve learned.</p>

<ul>
  <li>Stop thinking of t-SNE and UMAP as manifold learning methods.</li>
  <li>As with other visualization tools, use and interpret t-SNE and UMAP with caution.</li>
  <li>Try different visualization tools or diagnosis if possible.</li>
</ul>

<p>Interpreting visualization results inevitably involves some degree of subjectivity, and ultimately, it is up to the user to strike the right balance between expediency and rigor in scientific investigation.</p>

  </div><a class="u-url" href="/jekyll/update/2025/06/02/tsne-visualization.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Advancing Interpretability of Deep Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Advancing Interpretability of Deep Learning</li><li><a class="u-email" href="mailto:yiqiao.zhong@wisc.edu">yiqiao.zhong@wisc.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/Yiqiao-Zhong"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Yiqiao-Zhong</span></a></li><li><a href="https://www.twitter.com/yiqiao_zhong"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">yiqiao_zhong</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Can we understand the inner workings of black-box models? The goal of the blogs is to explore structures and analyze empirical phenoemna by scientific experiments on deep learning. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
